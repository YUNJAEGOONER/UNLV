{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras as ks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "data = pd.read_csv('data_day.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "del data['amount']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>complete</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>2002-05-07 06:00:00</td>\n",
       "      <td>127.920</td>\n",
       "      <td>127.920</td>\n",
       "      <td>127.920</td>\n",
       "      <td>127.920</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2002-05-08 06:00:00</td>\n",
       "      <td>128.920</td>\n",
       "      <td>128.920</td>\n",
       "      <td>128.920</td>\n",
       "      <td>128.920</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2002-05-09 06:00:00</td>\n",
       "      <td>128.380</td>\n",
       "      <td>128.380</td>\n",
       "      <td>128.380</td>\n",
       "      <td>128.380</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2002-05-10 06:00:00</td>\n",
       "      <td>127.630</td>\n",
       "      <td>127.630</td>\n",
       "      <td>127.630</td>\n",
       "      <td>127.630</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2002-05-11 06:00:00</td>\n",
       "      <td>127.620</td>\n",
       "      <td>127.620</td>\n",
       "      <td>127.620</td>\n",
       "      <td>127.620</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5430</td>\n",
       "      <td>2020-02-03 07:00:00</td>\n",
       "      <td>108.432</td>\n",
       "      <td>108.799</td>\n",
       "      <td>108.318</td>\n",
       "      <td>108.683</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5431</td>\n",
       "      <td>2020-02-04 07:00:00</td>\n",
       "      <td>108.688</td>\n",
       "      <td>109.546</td>\n",
       "      <td>108.550</td>\n",
       "      <td>109.525</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5432</td>\n",
       "      <td>2020-02-05 07:00:00</td>\n",
       "      <td>109.524</td>\n",
       "      <td>109.848</td>\n",
       "      <td>109.304</td>\n",
       "      <td>109.830</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5433</td>\n",
       "      <td>2020-02-06 07:00:00</td>\n",
       "      <td>109.824</td>\n",
       "      <td>110.004</td>\n",
       "      <td>109.743</td>\n",
       "      <td>109.996</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5434</td>\n",
       "      <td>2020-02-07 07:00:00</td>\n",
       "      <td>109.992</td>\n",
       "      <td>110.022</td>\n",
       "      <td>109.964</td>\n",
       "      <td>109.965</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5435 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     time     open     high      low    close  complete\n",
       "0     2002-05-07 06:00:00  127.920  127.920  127.920  127.920      True\n",
       "1     2002-05-08 06:00:00  128.920  128.920  128.920  128.920      True\n",
       "2     2002-05-09 06:00:00  128.380  128.380  128.380  128.380      True\n",
       "3     2002-05-10 06:00:00  127.630  127.630  127.630  127.630      True\n",
       "4     2002-05-11 06:00:00  127.620  127.620  127.620  127.620      True\n",
       "...                   ...      ...      ...      ...      ...       ...\n",
       "5430  2020-02-03 07:00:00  108.432  108.799  108.318  108.683      True\n",
       "5431  2020-02-04 07:00:00  108.688  109.546  108.550  109.525      True\n",
       "5432  2020-02-05 07:00:00  109.524  109.848  109.304  109.830      True\n",
       "5433  2020-02-06 07:00:00  109.824  110.004  109.743  109.996      True\n",
       "5434  2020-02-07 07:00:00  109.992  110.022  109.964  109.965     False\n",
       "\n",
       "[5435 rows x 6 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array(data.iloc[:5434,1:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array(data.iloc[1:,4:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[127.92 , 127.92 , 127.92 , 127.92 ],\n",
       "       [128.92 , 128.92 , 128.92 , 128.92 ],\n",
       "       [128.38 , 128.38 , 128.38 , 128.38 ],\n",
       "       ...,\n",
       "       [108.688, 109.546, 108.55 , 109.525],\n",
       "       [109.524, 109.848, 109.304, 109.83 ],\n",
       "       [109.824, 110.004, 109.743, 109.996]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[128.92 ],\n",
       "       [128.38 ],\n",
       "       [127.63 ],\n",
       "       ...,\n",
       "       [109.83 ],\n",
       "       [109.996],\n",
       "       [109.965]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y,train_size=0.8, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\2019A00303\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "WARNING:tensorflow:From C:\\Users\\2019A00303\\AppData\\Roaming\\Python\\Python37\\site-packages\\keras\\backend\\tensorflow_backend.py:4070: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 4, 32)             8192      \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 2, 32)             3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 1, 32)             0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 1, 8)              264       \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 8)                 0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                90        \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 11,661\n",
      "Trainable params: 11,661\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Embedding, MaxPooling1D, Flatten,MaxPooling1D\n",
    "from keras import models,layers\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(Embedding(256,32,input_length=4))\n",
    "model.add(layers.Conv1D(filters=32 ,kernel_size = 3, activation='relu'))\n",
    "\n",
    "model.add(layers.MaxPooling1D())\n",
    "model.add(layers.Conv1D(filters=8,kernel_size = 1, activation='relu'))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(layers.Dense(10, activation='softmax'))\n",
    "model.add(layers.Dense(1,activation='relu'))\n",
    "\n",
    "model.compile(optimizer='adam',loss='mse')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\2019A00303\\AppData\\Roaming\\Python\\Python37\\site-packages\\keras\\backend\\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Train on 3477 samples, validate on 870 samples\n",
      "Epoch 1/1200\n",
      "3477/3477 [==============================] - 1s 399us/step - loss: 11013.8240 - val_loss: 10796.8832\n",
      "Epoch 2/1200\n",
      "3477/3477 [==============================] - 0s 133us/step - loss: 10896.6602 - val_loss: 10742.8049\n",
      "Epoch 3/1200\n",
      "3477/3477 [==============================] - 1s 157us/step - loss: 10845.6938 - val_loss: 10694.2044\n",
      "Epoch 4/1200\n",
      "3477/3477 [==============================] - 0s 133us/step - loss: 10797.7279 - val_loss: 10647.2666\n",
      "Epoch 5/1200\n",
      "3477/3477 [==============================] - 0s 131us/step - loss: 10750.9861 - val_loss: 10601.2393\n",
      "Epoch 6/1200\n",
      "3477/3477 [==============================] - 1s 150us/step - loss: 10704.9457 - val_loss: 10555.7930\n",
      "Epoch 7/1200\n",
      "3477/3477 [==============================] - 0s 129us/step - loss: 10659.3758 - val_loss: 10510.7459\n",
      "Epoch 8/1200\n",
      "3477/3477 [==============================] - 0s 133us/step - loss: 10614.1789 - val_loss: 10466.0065\n",
      "Epoch 9/1200\n",
      "3477/3477 [==============================] - 0s 143us/step - loss: 10569.2894 - val_loss: 10421.5148\n",
      "Epoch 10/1200\n",
      "3477/3477 [==============================] - 1s 151us/step - loss: 10524.6551 - val_loss: 10377.3211\n",
      "Epoch 11/1200\n",
      "3477/3477 [==============================] - 1s 147us/step - loss: 10480.2441 - val_loss: 10333.3306\n",
      "Epoch 12/1200\n",
      "3477/3477 [==============================] - 1s 155us/step - loss: 10436.0425 - val_loss: 10289.5075\n",
      "Epoch 13/1200\n",
      "3477/3477 [==============================] - 0s 137us/step - loss: 10392.0213 - val_loss: 10245.8919\n",
      "Epoch 14/1200\n",
      "3477/3477 [==============================] - 1s 163us/step - loss: 10348.1753 - val_loss: 10202.3975\n",
      "Epoch 15/1200\n",
      "3477/3477 [==============================] - 0s 127us/step - loss: 10304.4846 - val_loss: 10159.1141\n",
      "Epoch 16/1200\n",
      "3477/3477 [==============================] - 0s 126us/step - loss: 10260.9593 - val_loss: 10115.9755\n",
      "Epoch 17/1200\n",
      "3477/3477 [==============================] - 0s 122us/step - loss: 10217.5737 - val_loss: 10072.9296\n",
      "Epoch 18/1200\n",
      "3477/3477 [==============================] - 0s 122us/step - loss: 10174.3288 - val_loss: 10030.0484\n",
      "Epoch 19/1200\n",
      "3477/3477 [==============================] - 0s 119us/step - loss: 10131.2220 - val_loss: 9987.3143\n",
      "Epoch 20/1200\n",
      "3477/3477 [==============================] - 0s 120us/step - loss: 10088.2473 - val_loss: 9944.6839\n",
      "Epoch 21/1200\n",
      "3477/3477 [==============================] - 0s 124us/step - loss: 10045.3946 - val_loss: 9902.2363\n",
      "Epoch 22/1200\n",
      "3477/3477 [==============================] - 0s 120us/step - loss: 10002.6599 - val_loss: 9859.8455\n",
      "Epoch 23/1200\n",
      "3477/3477 [==============================] - 0s 129us/step - loss: 9960.0500 - val_loss: 9817.5835\n",
      "Epoch 24/1200\n",
      "3477/3477 [==============================] - 0s 119us/step - loss: 9917.5592 - val_loss: 9775.4586\n",
      "Epoch 25/1200\n",
      "3477/3477 [==============================] - 0s 115us/step - loss: 9875.1882 - val_loss: 9733.4325\n",
      "Epoch 26/1200\n",
      "3477/3477 [==============================] - 0s 135us/step - loss: 9832.9260 - val_loss: 9691.5406\n",
      "Epoch 27/1200\n",
      "3477/3477 [==============================] - 0s 127us/step - loss: 9790.7737 - val_loss: 9649.7503\n",
      "Epoch 28/1200\n",
      "3477/3477 [==============================] - 0s 127us/step - loss: 9748.7333 - val_loss: 9608.0647\n",
      "Epoch 29/1200\n",
      "3477/3477 [==============================] - 0s 122us/step - loss: 9706.8136 - val_loss: 9566.4652\n",
      "Epoch 30/1200\n",
      "3477/3477 [==============================] - 0s 135us/step - loss: 9664.9871 - val_loss: 9525.0463\n",
      "Epoch 31/1200\n",
      "3477/3477 [==============================] - 1s 152us/step - loss: 9623.2759 - val_loss: 9483.6501\n",
      "Epoch 32/1200\n",
      "3477/3477 [==============================] - 0s 139us/step - loss: 9581.6628 - val_loss: 9442.3792\n",
      "Epoch 33/1200\n",
      "3477/3477 [==============================] - 0s 137us/step - loss: 9540.1641 - val_loss: 9401.2355\n",
      "Epoch 34/1200\n",
      "3477/3477 [==============================] - 1s 146us/step - loss: 9498.7672 - val_loss: 9360.1820\n",
      "Epoch 35/1200\n",
      "3477/3477 [==============================] - 0s 136us/step - loss: 9457.4737 - val_loss: 9319.2556\n",
      "Epoch 36/1200\n",
      "3477/3477 [==============================] - 0s 134us/step - loss: 9416.2785 - val_loss: 9278.4139\n",
      "Epoch 37/1200\n",
      "3477/3477 [==============================] - 1s 147us/step - loss: 9375.1830 - val_loss: 9237.6611\n",
      "Epoch 38/1200\n",
      "3477/3477 [==============================] - 0s 126us/step - loss: 9334.1879 - val_loss: 9197.0048\n",
      "Epoch 39/1200\n",
      "3477/3477 [==============================] - 1s 144us/step - loss: 9293.2892 - val_loss: 9156.4792\n",
      "Epoch 40/1200\n",
      "3477/3477 [==============================] - 0s 141us/step - loss: 9252.4900 - val_loss: 9116.0104\n",
      "Epoch 41/1200\n",
      "3477/3477 [==============================] - 0s 139us/step - loss: 9211.7909 - val_loss: 9075.6506\n",
      "Epoch 42/1200\n",
      "3477/3477 [==============================] - 1s 146us/step - loss: 9171.1883 - val_loss: 9035.4266\n",
      "Epoch 43/1200\n",
      "3477/3477 [==============================] - 1s 209us/step - loss: 9130.6999 - val_loss: 8995.2559\n",
      "Epoch 44/1200\n",
      "3477/3477 [==============================] - 0s 135us/step - loss: 9090.2952 - val_loss: 8955.1994\n",
      "Epoch 45/1200\n",
      "3477/3477 [==============================] - 1s 181us/step - loss: 9049.9927 - val_loss: 8915.2082\n",
      "Epoch 46/1200\n",
      "3477/3477 [==============================] - 1s 178us/step - loss: 9009.7827 - val_loss: 8875.4134\n",
      "Epoch 47/1200\n",
      "3477/3477 [==============================] - 1s 145us/step - loss: 8969.6802 - val_loss: 8835.6071\n",
      "Epoch 48/1200\n",
      "3477/3477 [==============================] - 0s 141us/step - loss: 8929.6709 - val_loss: 8795.9216\n",
      "Epoch 49/1200\n",
      "3477/3477 [==============================] - 0s 138us/step - loss: 8889.7484 - val_loss: 8756.4038\n",
      "Epoch 50/1200\n",
      "3477/3477 [==============================] - 0s 133us/step - loss: 8849.9285 - val_loss: 8716.9221\n",
      "Epoch 51/1200\n",
      "3477/3477 [==============================] - 0s 135us/step - loss: 8810.1972 - val_loss: 8677.5141\n",
      "Epoch 52/1200\n",
      "3477/3477 [==============================] - 1s 242us/step - loss: 8770.5627 - val_loss: 8638.2030\n",
      "Epoch 53/1200\n",
      "3477/3477 [==============================] - 0s 131us/step - loss: 8731.0206 - val_loss: 8599.0350\n",
      "Epoch 54/1200\n",
      "3477/3477 [==============================] - 0s 129us/step - loss: 8691.5762 - val_loss: 8559.9413\n",
      "Epoch 55/1200\n",
      "3477/3477 [==============================] - 0s 130us/step - loss: 8652.2216 - val_loss: 8520.9205\n",
      "Epoch 56/1200\n",
      "3477/3477 [==============================] - 0s 133us/step - loss: 8612.9669 - val_loss: 8481.9848\n",
      "Epoch 57/1200\n",
      "3477/3477 [==============================] - 0s 130us/step - loss: 8573.8050 - val_loss: 8443.1956\n",
      "Epoch 58/1200\n",
      "3477/3477 [==============================] - 0s 130us/step - loss: 8534.7402 - val_loss: 8404.4415\n",
      "Epoch 59/1200\n",
      "3477/3477 [==============================] - 0s 124us/step - loss: 8495.7681 - val_loss: 8365.8360\n",
      "Epoch 60/1200\n",
      "3477/3477 [==============================] - 1s 145us/step - loss: 8456.8900 - val_loss: 8327.3006\n",
      "Epoch 61/1200\n",
      "3477/3477 [==============================] - 0s 129us/step - loss: 8418.1003 - val_loss: 8288.8722\n",
      "Epoch 62/1200\n",
      "3477/3477 [==============================] - 0s 131us/step - loss: 8379.4144 - val_loss: 8250.4922\n",
      "Epoch 63/1200\n",
      "3477/3477 [==============================] - 1s 149us/step - loss: 8340.8155 - val_loss: 8212.2499\n",
      "Epoch 64/1200\n",
      "3477/3477 [==============================] - 0s 137us/step - loss: 8302.3121 - val_loss: 8174.0987\n",
      "Epoch 65/1200\n",
      "3477/3477 [==============================] - 0s 141us/step - loss: 8263.9050 - val_loss: 8136.0303\n",
      "Epoch 66/1200\n",
      "3477/3477 [==============================] - 1s 177us/step - loss: 8225.5842 - val_loss: 8098.0610\n",
      "Epoch 67/1200\n",
      "3477/3477 [==============================] - 1s 152us/step - loss: 8187.3595 - val_loss: 8060.1502\n",
      "Epoch 68/1200\n",
      "3477/3477 [==============================] - 1s 163us/step - loss: 8149.2367 - val_loss: 8022.3622\n",
      "Epoch 69/1200\n",
      "3477/3477 [==============================] - 1s 148us/step - loss: 8111.1979 - val_loss: 7984.7272\n",
      "Epoch 70/1200\n",
      "3477/3477 [==============================] - 0s 124us/step - loss: 8073.2610 - val_loss: 7947.0903\n",
      "Epoch 71/1200\n",
      "3477/3477 [==============================] - 0s 139us/step - loss: 8035.4067 - val_loss: 7909.5926\n",
      "Epoch 72/1200\n",
      "3477/3477 [==============================] - 0s 137us/step - loss: 7997.6472 - val_loss: 7872.2026\n",
      "Epoch 73/1200\n",
      "3477/3477 [==============================] - 0s 141us/step - loss: 7959.9858 - val_loss: 7834.8500\n",
      "Epoch 74/1200\n",
      "3477/3477 [==============================] - 0s 141us/step - loss: 7922.4123 - val_loss: 7797.6178\n",
      "Epoch 75/1200\n",
      "3477/3477 [==============================] - 1s 153us/step - loss: 7884.9372 - val_loss: 7760.4645\n",
      "Epoch 76/1200\n",
      "3477/3477 [==============================] - 0s 144us/step - loss: 7847.5492 - val_loss: 7723.4632\n",
      "Epoch 77/1200\n",
      "3477/3477 [==============================] - 1s 154us/step - loss: 7810.2628 - val_loss: 7686.4944\n",
      "Epoch 78/1200\n",
      "3477/3477 [==============================] - 1s 147us/step - loss: 7773.0664 - val_loss: 7649.6626\n",
      "Epoch 79/1200\n",
      "3477/3477 [==============================] - 1s 169us/step - loss: 7735.9577 - val_loss: 7612.9074\n",
      "Epoch 80/1200\n",
      "3477/3477 [==============================] - 0s 133us/step - loss: 7698.9474 - val_loss: 7576.1811\n",
      "Epoch 81/1200\n",
      "3477/3477 [==============================] - 0s 141us/step - loss: 7662.0226 - val_loss: 7539.6481\n",
      "Epoch 82/1200\n",
      "3477/3477 [==============================] - 0s 142us/step - loss: 7625.2015 - val_loss: 7503.1476\n",
      "Epoch 83/1200\n",
      "3477/3477 [==============================] - 0s 137us/step - loss: 7588.4685 - val_loss: 7466.7770\n",
      "Epoch 84/1200\n",
      "3477/3477 [==============================] - 0s 137us/step - loss: 7551.8367 - val_loss: 7430.4602\n",
      "Epoch 85/1200\n",
      "3477/3477 [==============================] - 0s 127us/step - loss: 7515.2949 - val_loss: 7394.2485\n",
      "Epoch 86/1200\n",
      "3477/3477 [==============================] - 0s 131us/step - loss: 7478.8371 - val_loss: 7358.2039\n",
      "Epoch 87/1200\n",
      "3477/3477 [==============================] - 0s 136us/step - loss: 7442.4775 - val_loss: 7322.1395\n",
      "Epoch 88/1200\n",
      "3477/3477 [==============================] - 0s 129us/step - loss: 7406.2095 - val_loss: 7286.2041\n",
      "Epoch 89/1200\n",
      "3477/3477 [==============================] - 1s 145us/step - loss: 7370.0313 - val_loss: 7250.3940\n",
      "Epoch 90/1200\n",
      "3477/3477 [==============================] - 0s 139us/step - loss: 7333.9555 - val_loss: 7214.6248\n",
      "Epoch 91/1200\n",
      "3477/3477 [==============================] - 0s 138us/step - loss: 7297.9640 - val_loss: 7179.0184\n",
      "Epoch 92/1200\n",
      "3477/3477 [==============================] - 1s 153us/step - loss: 7262.0722 - val_loss: 7143.4416\n",
      "Epoch 93/1200\n",
      "3477/3477 [==============================] - 0s 135us/step - loss: 7226.2686 - val_loss: 7107.9834\n",
      "Epoch 94/1200\n",
      "3477/3477 [==============================] - 0s 130us/step - loss: 7190.5543 - val_loss: 7072.6071\n",
      "Epoch 95/1200\n",
      "3477/3477 [==============================] - 0s 127us/step - loss: 7154.9358 - val_loss: 7037.3278\n",
      "Epoch 96/1200\n",
      "3477/3477 [==============================] - 0s 141us/step - loss: 7119.4060 - val_loss: 7002.1418\n",
      "Epoch 97/1200\n",
      "3477/3477 [==============================] - 1s 147us/step - loss: 7083.9666 - val_loss: 6967.0357\n",
      "Epoch 98/1200\n",
      "3477/3477 [==============================] - 0s 135us/step - loss: 7048.6383 - val_loss: 6932.0562\n",
      "Epoch 99/1200\n",
      "3477/3477 [==============================] - 0s 131us/step - loss: 7013.3939 - val_loss: 6897.1203\n",
      "Epoch 100/1200\n",
      "3477/3477 [==============================] - 0s 131us/step - loss: 6978.2310 - val_loss: 6862.3761\n",
      "Epoch 101/1200\n",
      "3477/3477 [==============================] - 0s 132us/step - loss: 6943.1737 - val_loss: 6827.6361\n",
      "Epoch 102/1200\n",
      "3477/3477 [==============================] - 0s 114us/step - loss: 6908.1998 - val_loss: 6793.0379\n",
      "Epoch 103/1200\n",
      "3477/3477 [==============================] - 0s 106us/step - loss: 6873.3285 - val_loss: 6758.4597\n",
      "Epoch 104/1200\n",
      "3477/3477 [==============================] - 0s 114us/step - loss: 6838.5375 - val_loss: 6724.0194\n",
      "Epoch 105/1200\n",
      "3477/3477 [==============================] - 0s 104us/step - loss: 6803.8553 - val_loss: 6689.6635\n",
      "Epoch 106/1200\n",
      "3477/3477 [==============================] - 0s 114us/step - loss: 6769.2647 - val_loss: 6655.4176\n",
      "Epoch 107/1200\n",
      "3477/3477 [==============================] - 0s 111us/step - loss: 6734.7641 - val_loss: 6621.2636\n",
      "Epoch 108/1200\n",
      "3477/3477 [==============================] - 0s 105us/step - loss: 6700.3499 - val_loss: 6587.2317\n",
      "Epoch 109/1200\n",
      "3477/3477 [==============================] - 0s 112us/step - loss: 6666.0306 - val_loss: 6553.2222\n",
      "Epoch 110/1200\n",
      "3477/3477 [==============================] - 0s 123us/step - loss: 6631.7977 - val_loss: 6519.3640\n",
      "Epoch 111/1200\n",
      "3477/3477 [==============================] - 0s 123us/step - loss: 6597.6749 - val_loss: 6485.5200\n",
      "Epoch 112/1200\n",
      "3477/3477 [==============================] - 0s 120us/step - loss: 6563.6348 - val_loss: 6451.8433\n",
      "Epoch 113/1200\n",
      "3477/3477 [==============================] - 0s 115us/step - loss: 6529.6974 - val_loss: 6418.2474\n",
      "Epoch 114/1200\n",
      "3477/3477 [==============================] - 1s 147us/step - loss: 6495.8410 - val_loss: 6384.7268\n",
      "Epoch 115/1200\n",
      "3477/3477 [==============================] - 0s 106us/step - loss: 6462.0809 - val_loss: 6351.3430\n",
      "Epoch 116/1200\n",
      "3477/3477 [==============================] - 0s 103us/step - loss: 6428.4096 - val_loss: 6317.9794\n",
      "Epoch 117/1200\n",
      "3477/3477 [==============================] - 0s 106us/step - loss: 6394.8238 - val_loss: 6284.7577\n",
      "Epoch 118/1200\n",
      "3477/3477 [==============================] - 0s 110us/step - loss: 6361.3400 - val_loss: 6251.6151\n",
      "Epoch 119/1200\n",
      "3477/3477 [==============================] - 0s 110us/step - loss: 6327.9529 - val_loss: 6218.5446\n",
      "Epoch 120/1200\n",
      "3477/3477 [==============================] - 0s 103us/step - loss: 6294.6457 - val_loss: 6185.6313\n",
      "Epoch 121/1200\n",
      "3477/3477 [==============================] - 0s 110us/step - loss: 6261.4411 - val_loss: 6152.7281\n",
      "Epoch 122/1200\n",
      "3477/3477 [==============================] - 0s 112us/step - loss: 6228.3199 - val_loss: 6119.9677\n",
      "Epoch 123/1200\n",
      "3477/3477 [==============================] - 0s 111us/step - loss: 6195.2991 - val_loss: 6087.2959\n",
      "Epoch 124/1200\n",
      "3477/3477 [==============================] - 0s 112us/step - loss: 6162.3656 - val_loss: 6054.7014\n",
      "Epoch 125/1200\n",
      "3477/3477 [==============================] - 0s 113us/step - loss: 6129.5260 - val_loss: 6022.2027\n",
      "Epoch 126/1200\n",
      "3477/3477 [==============================] - 0s 106us/step - loss: 6096.7839 - val_loss: 5989.7949\n",
      "Epoch 127/1200\n",
      "3477/3477 [==============================] - 0s 110us/step - loss: 6064.1313 - val_loss: 5957.4982\n",
      "Epoch 128/1200\n",
      "3477/3477 [==============================] - 0s 110us/step - loss: 6031.5753 - val_loss: 5925.2560\n",
      "Epoch 129/1200\n",
      "3477/3477 [==============================] - 0s 104us/step - loss: 5999.1033 - val_loss: 5893.1417\n",
      "Epoch 130/1200\n",
      "3477/3477 [==============================] - 0s 110us/step - loss: 5966.7300 - val_loss: 5861.0975\n",
      "Epoch 131/1200\n",
      "3477/3477 [==============================] - 0s 102us/step - loss: 5934.4449 - val_loss: 5829.1848\n",
      "Epoch 132/1200\n",
      "3477/3477 [==============================] - 0s 116us/step - loss: 5902.2680 - val_loss: 5797.3269\n",
      "Epoch 133/1200\n",
      "3477/3477 [==============================] - 0s 102us/step - loss: 5870.1795 - val_loss: 5765.5689\n",
      "Epoch 134/1200\n",
      "3477/3477 [==============================] - 0s 108us/step - loss: 5838.1775 - val_loss: 5733.9284\n",
      "Epoch 135/1200\n",
      "3477/3477 [==============================] - 0s 116us/step - loss: 5806.2694 - val_loss: 5702.3470\n",
      "Epoch 136/1200\n",
      "3477/3477 [==============================] - 0s 113us/step - loss: 5774.4555 - val_loss: 5670.8659\n",
      "Epoch 137/1200\n",
      "3477/3477 [==============================] - 0s 111us/step - loss: 5742.7304 - val_loss: 5639.5078\n",
      "Epoch 138/1200\n",
      "3477/3477 [==============================] - 0s 105us/step - loss: 5711.0972 - val_loss: 5608.2088\n",
      "Epoch 139/1200\n",
      "3477/3477 [==============================] - 0s 106us/step - loss: 5679.5552 - val_loss: 5577.0160\n",
      "Epoch 140/1200\n",
      "3477/3477 [==============================] - 0s 107us/step - loss: 5648.1120 - val_loss: 5545.8651\n",
      "Epoch 141/1200\n",
      "3477/3477 [==============================] - 0s 110us/step - loss: 5616.7540 - val_loss: 5514.8995\n",
      "Epoch 142/1200\n",
      "3477/3477 [==============================] - 0s 107us/step - loss: 5585.5008 - val_loss: 5483.9501\n",
      "Epoch 143/1200\n",
      "3477/3477 [==============================] - 0s 105us/step - loss: 5554.3343 - val_loss: 5453.1473\n",
      "Epoch 144/1200\n",
      "3477/3477 [==============================] - 0s 107us/step - loss: 5523.2668 - val_loss: 5422.4003\n",
      "Epoch 145/1200\n",
      "3477/3477 [==============================] - 0s 105us/step - loss: 5492.2811 - val_loss: 5391.7566\n",
      "Epoch 146/1200\n",
      "3477/3477 [==============================] - 0s 117us/step - loss: 5461.3828 - val_loss: 5361.1941\n",
      "Epoch 147/1200\n",
      "3477/3477 [==============================] - 0s 109us/step - loss: 5430.5867 - val_loss: 5330.7481\n",
      "Epoch 148/1200\n",
      "3477/3477 [==============================] - 0s 105us/step - loss: 5399.8740 - val_loss: 5300.4002\n",
      "Epoch 149/1200\n",
      "3477/3477 [==============================] - 0s 110us/step - loss: 5369.2559 - val_loss: 5270.1161\n",
      "Epoch 150/1200\n",
      "3477/3477 [==============================] - 0s 117us/step - loss: 5338.7306 - val_loss: 5239.9145\n",
      "Epoch 151/1200\n",
      "3477/3477 [==============================] - 0s 124us/step - loss: 5308.2949 - val_loss: 5209.8414\n",
      "Epoch 152/1200\n",
      "3477/3477 [==============================] - 0s 124us/step - loss: 5277.9586 - val_loss: 5179.8005\n",
      "Epoch 153/1200\n",
      "3477/3477 [==============================] - 0s 128us/step - loss: 5247.7043 - val_loss: 5149.9293\n",
      "Epoch 154/1200\n",
      "3477/3477 [==============================] - 0s 118us/step - loss: 5217.5482 - val_loss: 5120.1189\n",
      "Epoch 155/1200\n",
      "3477/3477 [==============================] - 0s 129us/step - loss: 5187.4911 - val_loss: 5090.3862\n",
      "Epoch 156/1200\n",
      "3477/3477 [==============================] - 0s 114us/step - loss: 5157.5222 - val_loss: 5060.7508\n",
      "Epoch 157/1200\n",
      "3477/3477 [==============================] - 0s 107us/step - loss: 5127.6434 - val_loss: 5031.2114\n",
      "Epoch 158/1200\n",
      "3477/3477 [==============================] - 0s 118us/step - loss: 5097.8567 - val_loss: 5001.7629\n",
      "Epoch 159/1200\n",
      "3477/3477 [==============================] - 0s 115us/step - loss: 5068.1664 - val_loss: 4972.4254\n",
      "Epoch 160/1200\n",
      "3477/3477 [==============================] - 0s 105us/step - loss: 5038.5695 - val_loss: 4943.1693\n",
      "Epoch 161/1200\n",
      "3477/3477 [==============================] - 0s 117us/step - loss: 5009.0606 - val_loss: 4914.0145\n",
      "Epoch 162/1200\n",
      "3477/3477 [==============================] - 0s 113us/step - loss: 4979.6501 - val_loss: 4884.9198\n",
      "Epoch 163/1200\n",
      "3477/3477 [==============================] - 0s 104us/step - loss: 4950.3266 - val_loss: 4855.9492\n",
      "Epoch 164/1200\n",
      "3477/3477 [==============================] - 0s 106us/step - loss: 4921.0992 - val_loss: 4827.0520\n",
      "Epoch 165/1200\n",
      "3477/3477 [==============================] - 0s 115us/step - loss: 4891.9634 - val_loss: 4798.2532\n",
      "Epoch 166/1200\n",
      "3477/3477 [==============================] - 0s 123us/step - loss: 4862.9207 - val_loss: 4769.5840\n",
      "Epoch 167/1200\n",
      "3477/3477 [==============================] - 0s 114us/step - loss: 4833.9698 - val_loss: 4740.9390\n",
      "Epoch 168/1200\n",
      "3477/3477 [==============================] - 0s 104us/step - loss: 4805.1132 - val_loss: 4712.4511\n",
      "Epoch 169/1200\n",
      "3477/3477 [==============================] - 0s 113us/step - loss: 4776.3472 - val_loss: 4683.9970\n",
      "Epoch 170/1200\n",
      "3477/3477 [==============================] - 0s 102us/step - loss: 4747.6709 - val_loss: 4655.6501\n",
      "Epoch 171/1200\n",
      "3477/3477 [==============================] - 0s 108us/step - loss: 4719.0854 - val_loss: 4627.4570\n",
      "Epoch 172/1200\n",
      "3477/3477 [==============================] - 0s 100us/step - loss: 4690.6053 - val_loss: 4599.2839\n",
      "Epoch 173/1200\n",
      "3477/3477 [==============================] - 0s 125us/step - loss: 4662.2131 - val_loss: 4571.2142\n",
      "Epoch 174/1200\n",
      "3477/3477 [==============================] - 0s 107us/step - loss: 4633.9067 - val_loss: 4543.2885\n",
      "Epoch 175/1200\n",
      "3477/3477 [==============================] - 0s 106us/step - loss: 4605.6939 - val_loss: 4515.4135\n",
      "Epoch 176/1200\n",
      "3477/3477 [==============================] - 0s 105us/step - loss: 4577.5736 - val_loss: 4487.5962\n",
      "Epoch 177/1200\n",
      "3477/3477 [==============================] - 0s 113us/step - loss: 4549.5444 - val_loss: 4459.9481\n",
      "Epoch 178/1200\n",
      "3477/3477 [==============================] - 0s 110us/step - loss: 4521.6046 - val_loss: 4432.3438\n",
      "Epoch 179/1200\n",
      "3477/3477 [==============================] - 0s 106us/step - loss: 4493.7546 - val_loss: 4404.8174\n",
      "Epoch 180/1200\n",
      "3477/3477 [==============================] - 0s 112us/step - loss: 4465.9978 - val_loss: 4377.4102\n",
      "Epoch 181/1200\n",
      "3477/3477 [==============================] - 0s 104us/step - loss: 4438.3377 - val_loss: 4350.1008\n",
      "Epoch 182/1200\n",
      "3477/3477 [==============================] - 0s 106us/step - loss: 4410.7666 - val_loss: 4322.8476\n",
      "Epoch 183/1200\n",
      "3477/3477 [==============================] - 0s 110us/step - loss: 4383.2852 - val_loss: 4295.6894\n",
      "Epoch 184/1200\n",
      "3477/3477 [==============================] - 0s 113us/step - loss: 4355.8942 - val_loss: 4268.6801\n",
      "Epoch 185/1200\n",
      "3477/3477 [==============================] - 0s 113us/step - loss: 4328.6030 - val_loss: 4241.6811\n",
      "Epoch 186/1200\n",
      "3477/3477 [==============================] - 0s 104us/step - loss: 4301.4113 - val_loss: 4214.8495\n",
      "Epoch 187/1200\n",
      "3477/3477 [==============================] - 0s 112us/step - loss: 4274.3084 - val_loss: 4188.1040\n",
      "Epoch 188/1200\n",
      "3477/3477 [==============================] - 0s 112us/step - loss: 4247.2903 - val_loss: 4161.4241\n",
      "Epoch 189/1200\n",
      "3477/3477 [==============================] - 0s 119us/step - loss: 4220.3656 - val_loss: 4134.8258\n",
      "Epoch 190/1200\n",
      "3477/3477 [==============================] - 0s 124us/step - loss: 4193.5314 - val_loss: 4108.3453\n",
      "Epoch 191/1200\n",
      "3477/3477 [==============================] - 0s 115us/step - loss: 4166.7843 - val_loss: 4081.9354\n",
      "Epoch 192/1200\n",
      "3477/3477 [==============================] - 1s 144us/step - loss: 4140.1422 - val_loss: 4055.5879\n",
      "Epoch 193/1200\n",
      "3477/3477 [==============================] - 0s 124us/step - loss: 4113.5838 - val_loss: 4029.4202\n",
      "Epoch 194/1200\n",
      "3477/3477 [==============================] - 0s 111us/step - loss: 4087.1218 - val_loss: 4003.3089\n",
      "Epoch 195/1200\n",
      "3477/3477 [==============================] - 0s 118us/step - loss: 4060.7473 - val_loss: 3977.2255\n",
      "Epoch 196/1200\n",
      "3477/3477 [==============================] - 0s 130us/step - loss: 4034.4664 - val_loss: 3951.2801\n",
      "Epoch 197/1200\n",
      "3477/3477 [==============================] - 0s 112us/step - loss: 4008.2791 - val_loss: 3925.4559\n",
      "Epoch 198/1200\n",
      "3477/3477 [==============================] - 0s 100us/step - loss: 3982.1822 - val_loss: 3899.7061\n",
      "Epoch 199/1200\n",
      "3477/3477 [==============================] - 0s 115us/step - loss: 3956.1761 - val_loss: 3874.0345\n",
      "Epoch 200/1200\n",
      "3477/3477 [==============================] - 0s 112us/step - loss: 3930.2621 - val_loss: 3848.4789\n",
      "Epoch 201/1200\n",
      "3477/3477 [==============================] - 0s 108us/step - loss: 3904.4470 - val_loss: 3822.9782\n",
      "Epoch 202/1200\n",
      "3477/3477 [==============================] - 0s 110us/step - loss: 3878.7158 - val_loss: 3797.5824\n",
      "Epoch 203/1200\n",
      "3477/3477 [==============================] - 0s 116us/step - loss: 3853.0752 - val_loss: 3772.3211\n",
      "Epoch 204/1200\n",
      "3477/3477 [==============================] - 0s 115us/step - loss: 3827.5332 - val_loss: 3747.0970\n",
      "Epoch 205/1200\n",
      "3477/3477 [==============================] - 0s 115us/step - loss: 3802.0846 - val_loss: 3721.9922\n",
      "Epoch 206/1200\n",
      "3477/3477 [==============================] - 0s 126us/step - loss: 3776.7293 - val_loss: 3696.9488\n",
      "Epoch 207/1200\n",
      "3477/3477 [==============================] - 0s 103us/step - loss: 3751.4518 - val_loss: 3672.0414\n",
      "Epoch 208/1200\n",
      "3477/3477 [==============================] - 0s 115us/step - loss: 3726.2742 - val_loss: 3647.1959\n",
      "Epoch 209/1200\n",
      "3477/3477 [==============================] - 0s 106us/step - loss: 3701.1862 - val_loss: 3622.4757\n",
      "Epoch 210/1200\n",
      "3477/3477 [==============================] - 0s 110us/step - loss: 3676.2021 - val_loss: 3597.7838\n",
      "Epoch 211/1200\n",
      "3477/3477 [==============================] - 0s 115us/step - loss: 3651.2929 - val_loss: 3573.2152\n",
      "Epoch 212/1200\n",
      "3477/3477 [==============================] - 0s 106us/step - loss: 3626.4837 - val_loss: 3548.7577\n",
      "Epoch 213/1200\n",
      "3477/3477 [==============================] - 0s 126us/step - loss: 3601.7755 - val_loss: 3524.3577\n",
      "Epoch 214/1200\n",
      "3477/3477 [==============================] - 0s 102us/step - loss: 3577.1531 - val_loss: 3500.1061\n",
      "Epoch 215/1200\n",
      "3477/3477 [==============================] - 0s 113us/step - loss: 3552.6235 - val_loss: 3475.9273\n",
      "Epoch 216/1200\n",
      "3477/3477 [==============================] - 0s 104us/step - loss: 3528.1837 - val_loss: 3451.8242\n",
      "Epoch 217/1200\n",
      "3477/3477 [==============================] - 0s 110us/step - loss: 3503.8390 - val_loss: 3427.7539\n",
      "Epoch 218/1200\n",
      "3477/3477 [==============================] - 0s 105us/step - loss: 3479.5772 - val_loss: 3403.8758\n",
      "Epoch 219/1200\n",
      "3477/3477 [==============================] - 0s 105us/step - loss: 3455.4122 - val_loss: 3380.0599\n",
      "Epoch 220/1200\n",
      "3477/3477 [==============================] - 0s 123us/step - loss: 3431.3336 - val_loss: 3356.3455\n",
      "Epoch 221/1200\n",
      "3477/3477 [==============================] - 0s 115us/step - loss: 3407.3520 - val_loss: 3332.6551\n",
      "Epoch 222/1200\n",
      "3477/3477 [==============================] - 0s 101us/step - loss: 3383.4601 - val_loss: 3309.1164\n",
      "Epoch 223/1200\n",
      "3477/3477 [==============================] - 0s 113us/step - loss: 3359.6657 - val_loss: 3285.6795\n",
      "Epoch 224/1200\n",
      "3477/3477 [==============================] - 0s 131us/step - loss: 3335.9614 - val_loss: 3262.2610\n",
      "Epoch 225/1200\n",
      "3477/3477 [==============================] - 0s 108us/step - loss: 3312.3406 - val_loss: 3239.0034\n",
      "Epoch 226/1200\n",
      "3477/3477 [==============================] - 0s 110us/step - loss: 3288.8162 - val_loss: 3215.8418\n",
      "Epoch 227/1200\n",
      "3477/3477 [==============================] - 0s 103us/step - loss: 3265.3797 - val_loss: 3192.7461\n",
      "Epoch 228/1200\n",
      "3477/3477 [==============================] - 0s 104us/step - loss: 3242.0465 - val_loss: 3169.7040\n",
      "Epoch 229/1200\n",
      "3477/3477 [==============================] - 0s 110us/step - loss: 3218.7866 - val_loss: 3146.8258\n",
      "Epoch 230/1200\n",
      "3477/3477 [==============================] - 0s 110us/step - loss: 3195.6299 - val_loss: 3123.9942\n",
      "Epoch 231/1200\n",
      "3477/3477 [==============================] - 0s 115us/step - loss: 3172.5668 - val_loss: 3101.2672\n",
      "Epoch 232/1200\n",
      "3477/3477 [==============================] - 0s 132us/step - loss: 3149.5936 - val_loss: 3078.6135\n",
      "Epoch 233/1200\n",
      "3477/3477 [==============================] - 1s 153us/step - loss: 3126.7071 - val_loss: 3056.0963\n",
      "Epoch 234/1200\n",
      "3477/3477 [==============================] - 0s 129us/step - loss: 3103.9210 - val_loss: 3033.6274\n",
      "Epoch 235/1200\n",
      "3477/3477 [==============================] - 0s 110us/step - loss: 3081.2247 - val_loss: 3011.2414\n",
      "Epoch 236/1200\n",
      "3477/3477 [==============================] - 0s 142us/step - loss: 3058.6153 - val_loss: 2989.0508\n",
      "Epoch 237/1200\n",
      "3477/3477 [==============================] - 0s 125us/step - loss: 3036.1027 - val_loss: 2966.8415\n",
      "Epoch 238/1200\n",
      "3477/3477 [==============================] - 0s 107us/step - loss: 3013.6754 - val_loss: 2944.7407\n",
      "Epoch 239/1200\n",
      "3477/3477 [==============================] - 0s 126us/step - loss: 2991.3448 - val_loss: 2922.7273\n",
      "Epoch 240/1200\n",
      "3477/3477 [==============================] - 0s 106us/step - loss: 2969.1151 - val_loss: 2900.8169\n",
      "Epoch 241/1200\n",
      "3477/3477 [==============================] - 0s 119us/step - loss: 2946.9612 - val_loss: 2879.0877\n",
      "Epoch 242/1200\n",
      "3477/3477 [==============================] - 0s 109us/step - loss: 2924.9097 - val_loss: 2857.3126\n",
      "Epoch 243/1200\n",
      "3477/3477 [==============================] - 0s 110us/step - loss: 2902.9453 - val_loss: 2835.6689\n",
      "Epoch 244/1200\n",
      "3477/3477 [==============================] - 0s 104us/step - loss: 2881.0638 - val_loss: 2814.1643\n",
      "Epoch 245/1200\n",
      "3477/3477 [==============================] - 0s 108us/step - loss: 2859.2853 - val_loss: 2792.7114\n",
      "Epoch 246/1200\n",
      "3477/3477 [==============================] - 0s 119us/step - loss: 2837.5974 - val_loss: 2771.3629\n",
      "Epoch 247/1200\n",
      "3477/3477 [==============================] - 0s 105us/step - loss: 2815.9913 - val_loss: 2750.1059\n",
      "Epoch 248/1200\n",
      "3477/3477 [==============================] - 0s 111us/step - loss: 2794.4850 - val_loss: 2728.9420\n",
      "Epoch 249/1200\n",
      "3477/3477 [==============================] - 0s 107us/step - loss: 2773.0717 - val_loss: 2707.8626\n",
      "Epoch 250/1200\n",
      "3477/3477 [==============================] - 0s 119us/step - loss: 2751.7411 - val_loss: 2686.8547\n",
      "Epoch 251/1200\n",
      "3477/3477 [==============================] - 0s 106us/step - loss: 2730.5101 - val_loss: 2665.9715\n",
      "Epoch 252/1200\n",
      "3477/3477 [==============================] - 0s 104us/step - loss: 2709.3769 - val_loss: 2645.1440\n",
      "Epoch 253/1200\n",
      "3477/3477 [==============================] - 0s 131us/step - loss: 2688.3212 - val_loss: 2624.4602\n",
      "Epoch 254/1200\n",
      "3477/3477 [==============================] - 0s 115us/step - loss: 2667.3637 - val_loss: 2603.8109\n",
      "Epoch 255/1200\n",
      "3477/3477 [==============================] - 0s 108us/step - loss: 2646.4941 - val_loss: 2583.2851\n",
      "Epoch 256/1200\n",
      "3477/3477 [==============================] - 0s 122us/step - loss: 2625.7162 - val_loss: 2562.8229\n",
      "Epoch 257/1200\n",
      "3477/3477 [==============================] - 0s 119us/step - loss: 2605.0363 - val_loss: 2542.4909\n",
      "Epoch 258/1200\n",
      "3477/3477 [==============================] - 0s 108us/step - loss: 2584.4393 - val_loss: 2522.2414\n",
      "Epoch 259/1200\n",
      "3477/3477 [==============================] - 0s 115us/step - loss: 2563.9256 - val_loss: 2502.0989\n",
      "Epoch 260/1200\n",
      "3477/3477 [==============================] - 0s 106us/step - loss: 2543.5133 - val_loss: 2481.9567\n",
      "Epoch 261/1200\n",
      "3477/3477 [==============================] - 0s 114us/step - loss: 2523.1850 - val_loss: 2462.0165\n",
      "Epoch 262/1200\n",
      "3477/3477 [==============================] - 0s 104us/step - loss: 2502.9535 - val_loss: 2442.1239\n",
      "Epoch 263/1200\n",
      "3477/3477 [==============================] - 0s 108us/step - loss: 2482.8127 - val_loss: 2422.3029\n",
      "Epoch 264/1200\n",
      "3477/3477 [==============================] - 0s 105us/step - loss: 2462.7605 - val_loss: 2402.5740\n",
      "Epoch 265/1200\n",
      "3477/3477 [==============================] - 0s 108us/step - loss: 2442.7960 - val_loss: 2382.9664\n",
      "Epoch 266/1200\n",
      "3477/3477 [==============================] - 0s 110us/step - loss: 2422.9315 - val_loss: 2363.4256\n",
      "Epoch 267/1200\n",
      "3477/3477 [==============================] - 0s 103us/step - loss: 2403.1449 - val_loss: 2344.0031\n",
      "Epoch 268/1200\n",
      "3477/3477 [==============================] - 0s 109us/step - loss: 2383.4626 - val_loss: 2324.6197\n",
      "Epoch 269/1200\n",
      "3477/3477 [==============================] - 0s 120us/step - loss: 2363.8693 - val_loss: 2305.3634\n",
      "Epoch 270/1200\n",
      "3477/3477 [==============================] - 0s 109us/step - loss: 2344.3655 - val_loss: 2286.2088\n",
      "Epoch 271/1200\n",
      "3477/3477 [==============================] - 0s 110us/step - loss: 2324.9497 - val_loss: 2267.1369\n",
      "Epoch 272/1200\n",
      "3477/3477 [==============================] - 0s 126us/step - loss: 2305.6289 - val_loss: 2248.1226\n",
      "Epoch 273/1200\n",
      "3477/3477 [==============================] - 0s 129us/step - loss: 2286.3969 - val_loss: 2229.2352\n",
      "Epoch 274/1200\n",
      "3477/3477 [==============================] - 0s 126us/step - loss: 2267.2638 - val_loss: 2210.4476\n",
      "Epoch 275/1200\n",
      "3477/3477 [==============================] - 0s 121us/step - loss: 2248.2165 - val_loss: 2191.7424\n",
      "Epoch 276/1200\n",
      "3477/3477 [==============================] - 1s 144us/step - loss: 2229.2558 - val_loss: 2173.1413\n",
      "Epoch 277/1200\n",
      "3477/3477 [==============================] - 0s 135us/step - loss: 2210.3991 - val_loss: 2154.5836\n",
      "Epoch 278/1200\n",
      "3477/3477 [==============================] - 0s 115us/step - loss: 2191.6221 - val_loss: 2136.1482\n",
      "Epoch 279/1200\n",
      "3477/3477 [==============================] - 0s 133us/step - loss: 2172.9346 - val_loss: 2117.8271\n",
      "Epoch 280/1200\n",
      "3477/3477 [==============================] - 0s 108us/step - loss: 2154.3377 - val_loss: 2099.5218\n",
      "Epoch 281/1200\n",
      "3477/3477 [==============================] - 0s 111us/step - loss: 2135.8233 - val_loss: 2081.3631\n",
      "Epoch 282/1200\n",
      "3477/3477 [==============================] - 0s 112us/step - loss: 2117.4120 - val_loss: 2063.3083\n",
      "Epoch 283/1200\n",
      "3477/3477 [==============================] - 0s 108us/step - loss: 2099.0920 - val_loss: 2045.3109\n",
      "Epoch 284/1200\n",
      "3477/3477 [==============================] - 0s 105us/step - loss: 2080.8529 - val_loss: 2027.4210\n",
      "Epoch 285/1200\n",
      "3477/3477 [==============================] - 0s 122us/step - loss: 2062.7122 - val_loss: 2009.6024\n",
      "Epoch 286/1200\n",
      "3477/3477 [==============================] - 0s 110us/step - loss: 2044.6528 - val_loss: 1991.8844\n",
      "Epoch 287/1200\n",
      "3477/3477 [==============================] - 0s 127us/step - loss: 2026.6938 - val_loss: 1974.2326\n",
      "Epoch 288/1200\n",
      "3477/3477 [==============================] - 0s 110us/step - loss: 2008.8253 - val_loss: 1956.6990\n",
      "Epoch 289/1200\n",
      "3477/3477 [==============================] - 0s 106us/step - loss: 1991.0460 - val_loss: 1939.2903\n",
      "Epoch 290/1200\n",
      "3477/3477 [==============================] - 0s 136us/step - loss: 1973.3651 - val_loss: 1921.9078\n",
      "Epoch 291/1200\n",
      "3477/3477 [==============================] - 0s 103us/step - loss: 1955.7679 - val_loss: 1904.6597\n",
      "Epoch 292/1200\n",
      "3477/3477 [==============================] - 0s 107us/step - loss: 1938.2637 - val_loss: 1887.4782\n",
      "Epoch 293/1200\n",
      "3477/3477 [==============================] - 0s 107us/step - loss: 1920.8498 - val_loss: 1870.4128\n",
      "Epoch 294/1200\n",
      "3477/3477 [==============================] - 0s 107us/step - loss: 1903.5290 - val_loss: 1853.4118\n",
      "Epoch 295/1200\n",
      "3477/3477 [==============================] - 0s 111us/step - loss: 1886.2939 - val_loss: 1836.5004\n",
      "Epoch 296/1200\n",
      "3477/3477 [==============================] - 0s 115us/step - loss: 1869.1472 - val_loss: 1819.6803\n",
      "Epoch 297/1200\n",
      "3477/3477 [==============================] - 0s 117us/step - loss: 1852.0988 - val_loss: 1802.9835\n",
      "Epoch 298/1200\n",
      "3477/3477 [==============================] - 0s 124us/step - loss: 1835.1434 - val_loss: 1786.3186\n",
      "Epoch 299/1200\n",
      "3477/3477 [==============================] - 0s 107us/step - loss: 1818.2708 - val_loss: 1769.8417\n",
      "Epoch 300/1200\n",
      "3477/3477 [==============================] - 0s 104us/step - loss: 1801.4967 - val_loss: 1753.3739\n",
      "Epoch 301/1200\n",
      "3477/3477 [==============================] - 0s 136us/step - loss: 1784.8058 - val_loss: 1737.0088\n",
      "Epoch 302/1200\n",
      "3477/3477 [==============================] - 0s 127us/step - loss: 1768.1990 - val_loss: 1720.7783\n",
      "Epoch 303/1200\n",
      "3477/3477 [==============================] - 0s 111us/step - loss: 1751.6865 - val_loss: 1704.5796\n",
      "Epoch 304/1200\n",
      "3477/3477 [==============================] - 0s 121us/step - loss: 1735.2673 - val_loss: 1688.4855\n",
      "Epoch 305/1200\n",
      "3477/3477 [==============================] - 0s 129us/step - loss: 1718.9341 - val_loss: 1672.5000\n",
      "Epoch 306/1200\n",
      "3477/3477 [==============================] - 0s 103us/step - loss: 1702.6923 - val_loss: 1656.5724\n",
      "Epoch 307/1200\n",
      "3477/3477 [==============================] - 0s 107us/step - loss: 1686.5376 - val_loss: 1640.7925\n",
      "Epoch 308/1200\n",
      "3477/3477 [==============================] - 0s 107us/step - loss: 1670.4800 - val_loss: 1625.0301\n",
      "Epoch 309/1200\n",
      "3477/3477 [==============================] - 0s 121us/step - loss: 1654.5090 - val_loss: 1609.4114\n",
      "Epoch 310/1200\n",
      "3477/3477 [==============================] - 0s 105us/step - loss: 1638.6245 - val_loss: 1593.8396\n",
      "Epoch 311/1200\n",
      "3477/3477 [==============================] - 0s 141us/step - loss: 1622.8307 - val_loss: 1578.4050\n",
      "Epoch 312/1200\n",
      "3477/3477 [==============================] - 0s 143us/step - loss: 1607.1343 - val_loss: 1562.9995\n",
      "Epoch 313/1200\n",
      "3477/3477 [==============================] - 0s 124us/step - loss: 1591.5211 - val_loss: 1547.7487\n",
      "Epoch 314/1200\n",
      "3477/3477 [==============================] - 0s 129us/step - loss: 1576.0022 - val_loss: 1532.5631\n",
      "Epoch 315/1200\n",
      "3477/3477 [==============================] - 1s 149us/step - loss: 1560.5723 - val_loss: 1517.4544\n",
      "Epoch 316/1200\n",
      "3477/3477 [==============================] - 0s 111us/step - loss: 1545.2278 - val_loss: 1502.4688\n",
      "Epoch 317/1200\n",
      "3477/3477 [==============================] - 0s 107us/step - loss: 1529.9803 - val_loss: 1487.5291\n",
      "Epoch 318/1200\n",
      "3477/3477 [==============================] - 0s 88us/step - loss: 1514.8178 - val_loss: 1472.6992\n",
      "Epoch 319/1200\n",
      "3477/3477 [==============================] - 0s 84us/step - loss: 1499.7472 - val_loss: 1457.9732\n",
      "Epoch 320/1200\n",
      "3477/3477 [==============================] - 0s 80us/step - loss: 1484.7687 - val_loss: 1443.3238\n",
      "Epoch 321/1200\n",
      "3477/3477 [==============================] - 0s 92us/step - loss: 1469.8763 - val_loss: 1428.7866\n",
      "Epoch 322/1200\n",
      "3477/3477 [==============================] - 0s 82us/step - loss: 1455.0819 - val_loss: 1414.2865\n",
      "Epoch 323/1200\n",
      "3477/3477 [==============================] - 0s 80us/step - loss: 1440.3675 - val_loss: 1399.9187\n",
      "Epoch 324/1200\n",
      "3477/3477 [==============================] - 0s 81us/step - loss: 1425.7532 - val_loss: 1385.6085\n",
      "Epoch 325/1200\n",
      "3477/3477 [==============================] - 0s 82us/step - loss: 1411.2221 - val_loss: 1371.4309\n",
      "Epoch 326/1200\n",
      "3477/3477 [==============================] - 0s 91us/step - loss: 1396.7862 - val_loss: 1357.3203\n",
      "Epoch 327/1200\n",
      "3477/3477 [==============================] - 0s 81us/step - loss: 1382.4348 - val_loss: 1343.2990\n",
      "Epoch 328/1200\n",
      "3477/3477 [==============================] - 0s 92us/step - loss: 1368.1710 - val_loss: 1329.3544\n",
      "Epoch 329/1200\n",
      "3477/3477 [==============================] - 0s 83us/step - loss: 1353.9970 - val_loss: 1315.5399\n",
      "Epoch 330/1200\n",
      "3477/3477 [==============================] - 0s 84us/step - loss: 1339.9181 - val_loss: 1301.7673\n",
      "Epoch 331/1200\n",
      "3477/3477 [==============================] - 0s 93us/step - loss: 1325.9252 - val_loss: 1288.1059\n",
      "Epoch 332/1200\n",
      "3477/3477 [==============================] - 0s 83us/step - loss: 1312.0198 - val_loss: 1274.5424\n",
      "Epoch 333/1200\n",
      "3477/3477 [==============================] - 0s 80us/step - loss: 1298.2103 - val_loss: 1261.0665\n",
      "Epoch 334/1200\n",
      "3477/3477 [==============================] - 0s 83us/step - loss: 1284.4873 - val_loss: 1247.6532\n",
      "Epoch 335/1200\n",
      "3477/3477 [==============================] - 0s 87us/step - loss: 1270.8486 - val_loss: 1234.3652\n",
      "Epoch 336/1200\n",
      "3477/3477 [==============================] - 0s 83us/step - loss: 1257.2950 - val_loss: 1221.1644\n",
      "Epoch 337/1200\n",
      "3477/3477 [==============================] - 0s 82us/step - loss: 1243.8350 - val_loss: 1208.0041\n",
      "Epoch 338/1200\n",
      "3477/3477 [==============================] - 0s 82us/step - loss: 1230.4654 - val_loss: 1194.9325\n",
      "Epoch 339/1200\n",
      "3477/3477 [==============================] - 0s 87us/step - loss: 1217.1868 - val_loss: 1182.0445\n",
      "Epoch 340/1200\n",
      "3477/3477 [==============================] - 0s 84us/step - loss: 1204.0072 - val_loss: 1169.1580\n",
      "Epoch 341/1200\n",
      "3477/3477 [==============================] - 0s 83us/step - loss: 1190.8963 - val_loss: 1156.4148\n",
      "Epoch 342/1200\n",
      "3477/3477 [==============================] - 0s 81us/step - loss: 1177.8900 - val_loss: 1143.7282\n",
      "Epoch 343/1200\n",
      "3477/3477 [==============================] - 0s 85us/step - loss: 1164.9748 - val_loss: 1131.1135\n",
      "Epoch 344/1200\n",
      "3477/3477 [==============================] - 0s 82us/step - loss: 1152.1479 - val_loss: 1118.6323\n",
      "Epoch 345/1200\n",
      "3477/3477 [==============================] - 0s 81us/step - loss: 1139.4086 - val_loss: 1106.2191\n",
      "Epoch 346/1200\n",
      "3477/3477 [==============================] - 0s 80us/step - loss: 1126.7594 - val_loss: 1093.8843\n",
      "Epoch 347/1200\n",
      "3477/3477 [==============================] - 0s 84us/step - loss: 1114.1898 - val_loss: 1081.6743\n",
      "Epoch 348/1200\n",
      "3477/3477 [==============================] - 0s 81us/step - loss: 1101.7144 - val_loss: 1069.5107\n",
      "Epoch 349/1200\n",
      "3477/3477 [==============================] - 0s 86us/step - loss: 1089.3244 - val_loss: 1057.4435\n",
      "Epoch 350/1200\n",
      "3477/3477 [==============================] - 0s 83us/step - loss: 1077.0223 - val_loss: 1045.4921\n",
      "Epoch 351/1200\n",
      "3477/3477 [==============================] - 0s 82us/step - loss: 1064.8170 - val_loss: 1033.5850\n",
      "Epoch 352/1200\n",
      "3477/3477 [==============================] - 0s 97us/step - loss: 1052.6913 - val_loss: 1021.7952\n",
      "Epoch 353/1200\n",
      "3477/3477 [==============================] - 0s 83us/step - loss: 1040.6624 - val_loss: 1010.1026\n",
      "Epoch 354/1200\n",
      "3477/3477 [==============================] - 0s 80us/step - loss: 1028.7248 - val_loss: 998.4802\n",
      "Epoch 355/1200\n",
      "3477/3477 [==============================] - 0s 82us/step - loss: 1016.8716 - val_loss: 986.9746\n",
      "Epoch 356/1200\n",
      "3477/3477 [==============================] - 0s 85us/step - loss: 1005.1103 - val_loss: 975.5246\n",
      "Epoch 357/1200\n",
      "3477/3477 [==============================] - 0s 84us/step - loss: 993.4270 - val_loss: 964.2027\n",
      "Epoch 358/1200\n",
      "3477/3477 [==============================] - 0s 84us/step - loss: 981.8505 - val_loss: 952.9179\n",
      "Epoch 359/1200\n",
      "3477/3477 [==============================] - 0s 84us/step - loss: 970.3379 - val_loss: 941.7724\n",
      "Epoch 360/1200\n",
      "3477/3477 [==============================] - 0s 82us/step - loss: 958.9208 - val_loss: 930.6728\n",
      "Epoch 361/1200\n",
      "3477/3477 [==============================] - 0s 83us/step - loss: 947.5937 - val_loss: 919.6414\n",
      "Epoch 362/1200\n",
      "3477/3477 [==============================] - 0s 97us/step - loss: 936.3585 - val_loss: 908.7331\n",
      "Epoch 363/1200\n",
      "3477/3477 [==============================] - 0s 102us/step - loss: 925.2134 - val_loss: 897.9229\n",
      "Epoch 364/1200\n",
      "3477/3477 [==============================] - 0s 100us/step - loss: 914.1562 - val_loss: 887.1765\n",
      "Epoch 365/1200\n",
      "3477/3477 [==============================] - 0s 108us/step - loss: 903.1873 - val_loss: 876.5366\n",
      "Epoch 366/1200\n",
      "3477/3477 [==============================] - 0s 97us/step - loss: 892.3074 - val_loss: 865.9734\n",
      "Epoch 367/1200\n",
      "3477/3477 [==============================] - 0s 92us/step - loss: 881.5132 - val_loss: 855.5280\n",
      "Epoch 368/1200\n",
      "3477/3477 [==============================] - 0s 89us/step - loss: 870.8152 - val_loss: 845.1371\n",
      "Epoch 369/1200\n",
      "3477/3477 [==============================] - 0s 89us/step - loss: 860.1956 - val_loss: 834.8678\n",
      "Epoch 370/1200\n",
      "3477/3477 [==============================] - 0s 85us/step - loss: 849.6627 - val_loss: 824.6456\n",
      "Epoch 371/1200\n",
      "3477/3477 [==============================] - 0s 82us/step - loss: 839.2234 - val_loss: 814.5504\n",
      "Epoch 372/1200\n",
      "3477/3477 [==============================] - 0s 112us/step - loss: 828.8694 - val_loss: 804.5109\n",
      "Epoch 373/1200\n",
      "3477/3477 [==============================] - 0s 107us/step - loss: 818.6016 - val_loss: 794.5870\n",
      "Epoch 374/1200\n",
      "3477/3477 [==============================] - 0s 95us/step - loss: 808.4255 - val_loss: 784.7132\n",
      "Epoch 375/1200\n",
      "3477/3477 [==============================] - 1s 156us/step - loss: 798.3354 - val_loss: 774.9470\n",
      "Epoch 376/1200\n",
      "3477/3477 [==============================] - 0s 105us/step - loss: 788.3343 - val_loss: 765.2738\n",
      "Epoch 377/1200\n",
      "3477/3477 [==============================] - 0s 100us/step - loss: 778.4191 - val_loss: 755.6883\n",
      "Epoch 378/1200\n",
      "3477/3477 [==============================] - 1s 168us/step - loss: 768.5827 - val_loss: 746.1946\n",
      "Epoch 379/1200\n",
      "3477/3477 [==============================] - 1s 157us/step - loss: 758.8403 - val_loss: 736.7476\n",
      "Epoch 380/1200\n",
      "3477/3477 [==============================] - 0s 108us/step - loss: 749.1874 - val_loss: 727.4026\n",
      "Epoch 381/1200\n",
      "3477/3477 [==============================] - 0s 89us/step - loss: 739.6185 - val_loss: 718.1666\n",
      "Epoch 382/1200\n",
      "3477/3477 [==============================] - 0s 85us/step - loss: 730.1402 - val_loss: 709.0064\n",
      "Epoch 383/1200\n",
      "3477/3477 [==============================] - 0s 86us/step - loss: 720.7493 - val_loss: 699.9311\n",
      "Epoch 384/1200\n",
      "3477/3477 [==============================] - 0s 86us/step - loss: 711.4439 - val_loss: 690.9649\n",
      "Epoch 385/1200\n",
      "3477/3477 [==============================] - 0s 82us/step - loss: 702.2230 - val_loss: 682.0782\n",
      "Epoch 386/1200\n",
      "3477/3477 [==============================] - 0s 89us/step - loss: 693.0942 - val_loss: 673.2516\n",
      "Epoch 387/1200\n",
      "3477/3477 [==============================] - 0s 87us/step - loss: 684.0499 - val_loss: 664.5399\n",
      "Epoch 388/1200\n",
      "3477/3477 [==============================] - 0s 87us/step - loss: 675.0968 - val_loss: 655.8975\n",
      "Epoch 389/1200\n",
      "3477/3477 [==============================] - 0s 83us/step - loss: 666.2304 - val_loss: 647.3483\n",
      "Epoch 390/1200\n",
      "3477/3477 [==============================] - ETA: 0s - loss: 657.268 - 0s 86us/step - loss: 657.4445 - val_loss: 638.8968\n",
      "Epoch 391/1200\n",
      "3477/3477 [==============================] - 0s 98us/step - loss: 648.7463 - val_loss: 630.5166\n",
      "Epoch 392/1200\n",
      "3477/3477 [==============================] - 0s 92us/step - loss: 640.1290 - val_loss: 622.2097\n",
      "Epoch 393/1200\n",
      "3477/3477 [==============================] - 0s 89us/step - loss: 631.5981 - val_loss: 613.9887\n",
      "Epoch 394/1200\n",
      "3477/3477 [==============================] - 0s 82us/step - loss: 623.1602 - val_loss: 605.8921\n",
      "Epoch 395/1200\n",
      "3477/3477 [==============================] - 0s 91us/step - loss: 614.7988 - val_loss: 597.8379\n",
      "Epoch 396/1200\n",
      "3477/3477 [==============================] - 0s 87us/step - loss: 606.5278 - val_loss: 589.9137\n",
      "Epoch 397/1200\n",
      "3477/3477 [==============================] - 0s 85us/step - loss: 598.3460 - val_loss: 582.0294\n",
      "Epoch 398/1200\n",
      "3477/3477 [==============================] - 0s 90us/step - loss: 590.2554 - val_loss: 574.2470\n",
      "Epoch 399/1200\n",
      "3477/3477 [==============================] - 0s 84us/step - loss: 582.2425 - val_loss: 566.5711\n",
      "Epoch 400/1200\n",
      "3477/3477 [==============================] - 0s 87us/step - loss: 574.3222 - val_loss: 558.9466\n",
      "Epoch 401/1200\n",
      "3477/3477 [==============================] - 0s 83us/step - loss: 566.4780 - val_loss: 551.4620\n",
      "Epoch 402/1200\n",
      "3477/3477 [==============================] - 0s 82us/step - loss: 558.7263 - val_loss: 543.9835\n",
      "Epoch 403/1200\n",
      "3477/3477 [==============================] - 0s 82us/step - loss: 551.0617 - val_loss: 536.6355\n",
      "Epoch 404/1200\n",
      "3477/3477 [==============================] - 0s 83us/step - loss: 543.4823 - val_loss: 529.3636\n",
      "Epoch 405/1200\n",
      "3477/3477 [==============================] - 0s 84us/step - loss: 535.9930 - val_loss: 522.2068\n",
      "Epoch 406/1200\n",
      "3477/3477 [==============================] - 0s 91us/step - loss: 528.5871 - val_loss: 515.1266\n",
      "Epoch 407/1200\n",
      "3477/3477 [==============================] - 0s 91us/step - loss: 521.2632 - val_loss: 508.1104\n",
      "Epoch 408/1200\n",
      "3477/3477 [==============================] - 0s 87us/step - loss: 514.0250 - val_loss: 501.2001\n",
      "Epoch 409/1200\n",
      "3477/3477 [==============================] - 0s 87us/step - loss: 506.8683 - val_loss: 494.3904\n",
      "Epoch 410/1200\n",
      "3477/3477 [==============================] - 0s 97us/step - loss: 499.8077 - val_loss: 487.5905\n",
      "Epoch 411/1200\n",
      "3477/3477 [==============================] - 0s 108us/step - loss: 492.8227 - val_loss: 480.9346\n",
      "Epoch 412/1200\n",
      "3477/3477 [==============================] - 0s 105us/step - loss: 485.9222 - val_loss: 474.3570\n",
      "Epoch 413/1200\n",
      "3477/3477 [==============================] - 0s 95us/step - loss: 479.1013 - val_loss: 467.8394\n",
      "Epoch 414/1200\n",
      "3477/3477 [==============================] - 0s 88us/step - loss: 472.3640 - val_loss: 461.4024\n",
      "Epoch 415/1200\n",
      "3477/3477 [==============================] - 0s 99us/step - loss: 465.7133 - val_loss: 455.0719\n",
      "Epoch 416/1200\n",
      "3477/3477 [==============================] - 0s 87us/step - loss: 459.1480 - val_loss: 448.8489\n",
      "Epoch 417/1200\n",
      "3477/3477 [==============================] - 0s 86us/step - loss: 452.6616 - val_loss: 442.6503\n",
      "Epoch 418/1200\n",
      "3477/3477 [==============================] - 0s 81us/step - loss: 446.2630 - val_loss: 436.5480\n",
      "Epoch 419/1200\n",
      "3477/3477 [==============================] - 0s 80us/step - loss: 439.9452 - val_loss: 430.5608\n",
      "Epoch 420/1200\n",
      "3477/3477 [==============================] - 0s 80us/step - loss: 433.7110 - val_loss: 424.6428\n",
      "Epoch 421/1200\n",
      "3477/3477 [==============================] - 0s 80us/step - loss: 427.5659 - val_loss: 418.7836\n",
      "Epoch 422/1200\n",
      "3477/3477 [==============================] - 0s 81us/step - loss: 421.5022 - val_loss: 413.0436\n",
      "Epoch 423/1200\n",
      "3477/3477 [==============================] - 0s 83us/step - loss: 415.5185 - val_loss: 407.3809\n",
      "Epoch 424/1200\n",
      "3477/3477 [==============================] - 0s 81us/step - loss: 409.6245 - val_loss: 401.7689\n",
      "Epoch 425/1200\n",
      "3477/3477 [==============================] - 0s 81us/step - loss: 403.8104 - val_loss: 396.2793\n",
      "Epoch 426/1200\n",
      "3477/3477 [==============================] - 0s 87us/step - loss: 398.0815 - val_loss: 390.8533\n",
      "Epoch 427/1200\n",
      "3477/3477 [==============================] - 0s 80us/step - loss: 392.4343 - val_loss: 385.5213\n",
      "Epoch 428/1200\n",
      "3477/3477 [==============================] - 0s 82us/step - loss: 386.8702 - val_loss: 380.2378\n",
      "Epoch 429/1200\n",
      "3477/3477 [==============================] - 0s 88us/step - loss: 381.3869 - val_loss: 375.0757\n",
      "Epoch 430/1200\n",
      "3477/3477 [==============================] - 0s 83us/step - loss: 375.9781 - val_loss: 369.9865\n",
      "Epoch 431/1200\n",
      "3477/3477 [==============================] - 0s 106us/step - loss: 370.6575 - val_loss: 364.9566\n",
      "Epoch 432/1200\n",
      "3477/3477 [==============================] - 0s 93us/step - loss: 365.4167 - val_loss: 360.0522\n",
      "Epoch 433/1200\n",
      "3477/3477 [==============================] - 0s 93us/step - loss: 360.2639 - val_loss: 355.1826\n",
      "Epoch 434/1200\n",
      "3477/3477 [==============================] - 0s 93us/step - loss: 355.1862 - val_loss: 350.4037\n",
      "Epoch 435/1200\n",
      "3477/3477 [==============================] - 0s 94us/step - loss: 350.1924 - val_loss: 345.6801\n",
      "Epoch 436/1200\n",
      "3477/3477 [==============================] - 0s 86us/step - loss: 345.2825 - val_loss: 341.1058\n",
      "Epoch 437/1200\n",
      "3477/3477 [==============================] - 0s 91us/step - loss: 340.4532 - val_loss: 336.5861\n",
      "Epoch 438/1200\n",
      "3477/3477 [==============================] - 0s 91us/step - loss: 335.7039 - val_loss: 332.1055\n",
      "Epoch 439/1200\n",
      "3477/3477 [==============================] - 0s 91us/step - loss: 331.0193 - val_loss: 327.7635\n",
      "Epoch 440/1200\n",
      "3477/3477 [==============================] - 0s 90us/step - loss: 326.4284 - val_loss: 323.4476\n",
      "Epoch 441/1200\n",
      "3477/3477 [==============================] - 0s 87us/step - loss: 321.9179 - val_loss: 319.2344\n",
      "Epoch 442/1200\n",
      "3477/3477 [==============================] - 0s 93us/step - loss: 317.4862 - val_loss: 315.1077\n",
      "Epoch 443/1200\n",
      "3477/3477 [==============================] - 0s 82us/step - loss: 313.1308 - val_loss: 311.0661\n",
      "Epoch 444/1200\n",
      "3477/3477 [==============================] - 0s 81us/step - loss: 308.8628 - val_loss: 307.0767\n",
      "Epoch 445/1200\n",
      "3477/3477 [==============================] - 0s 87us/step - loss: 304.6683 - val_loss: 303.1971\n",
      "Epoch 446/1200\n",
      "3477/3477 [==============================] - 0s 86us/step - loss: 300.5521 - val_loss: 299.3701\n",
      "Epoch 447/1200\n",
      "3477/3477 [==============================] - 0s 98us/step - loss: 296.5161 - val_loss: 295.6207\n",
      "Epoch 448/1200\n",
      "3477/3477 [==============================] - 0s 82us/step - loss: 292.5524 - val_loss: 291.9485\n",
      "Epoch 449/1200\n",
      "3477/3477 [==============================] - 0s 83us/step - loss: 288.6693 - val_loss: 288.3727\n",
      "Epoch 450/1200\n",
      "3477/3477 [==============================] - 0s 82us/step - loss: 284.8673 - val_loss: 284.8869\n",
      "Epoch 451/1200\n",
      "3477/3477 [==============================] - 0s 86us/step - loss: 281.1459 - val_loss: 281.4158\n",
      "Epoch 452/1200\n",
      "3477/3477 [==============================] - 0s 86us/step - loss: 277.4943 - val_loss: 278.0584\n",
      "Epoch 453/1200\n",
      "3477/3477 [==============================] - 0s 83us/step - loss: 273.9298 - val_loss: 274.7748\n",
      "Epoch 454/1200\n",
      "3477/3477 [==============================] - 0s 82us/step - loss: 270.4366 - val_loss: 271.5881\n",
      "Epoch 455/1200\n",
      "3477/3477 [==============================] - 0s 86us/step - loss: 267.0215 - val_loss: 268.4509\n",
      "Epoch 456/1200\n",
      "3477/3477 [==============================] - 0s 81us/step - loss: 263.6801 - val_loss: 265.3908\n",
      "Epoch 457/1200\n",
      "3477/3477 [==============================] - 0s 82us/step - loss: 260.4129 - val_loss: 262.4115\n",
      "Epoch 458/1200\n",
      "3477/3477 [==============================] - 0s 81us/step - loss: 257.2344 - val_loss: 259.4941\n",
      "Epoch 459/1200\n",
      "3477/3477 [==============================] - 0s 81us/step - loss: 254.1182 - val_loss: 256.6744\n",
      "Epoch 460/1200\n",
      "3477/3477 [==============================] - 0s 94us/step - loss: 251.0802 - val_loss: 253.9234\n",
      "Epoch 461/1200\n",
      "3477/3477 [==============================] - 0s 79us/step - loss: 248.1158 - val_loss: 251.2779\n",
      "Epoch 462/1200\n",
      "3477/3477 [==============================] - 0s 82us/step - loss: 245.2214 - val_loss: 248.6447\n",
      "Epoch 463/1200\n",
      "3477/3477 [==============================] - 0s 97us/step - loss: 242.3973 - val_loss: 246.0659\n",
      "Epoch 464/1200\n",
      "3477/3477 [==============================] - 0s 119us/step - loss: 239.6498 - val_loss: 243.5916\n",
      "Epoch 465/1200\n",
      "3477/3477 [==============================] - 0s 108us/step - loss: 236.9794 - val_loss: 241.2160\n",
      "Epoch 466/1200\n",
      "3477/3477 [==============================] - 0s 87us/step - loss: 234.3790 - val_loss: 238.8704\n",
      "Epoch 467/1200\n",
      "3477/3477 [==============================] - 0s 97us/step - loss: 231.8523 - val_loss: 236.6372\n",
      "Epoch 468/1200\n",
      "3477/3477 [==============================] - 0s 95us/step - loss: 229.3950 - val_loss: 234.4653\n",
      "Epoch 469/1200\n",
      "3477/3477 [==============================] - 0s 89us/step - loss: 227.0065 - val_loss: 232.3276\n",
      "Epoch 470/1200\n",
      "3477/3477 [==============================] - 0s 82us/step - loss: 224.6886 - val_loss: 230.2591\n",
      "Epoch 471/1200\n",
      "3477/3477 [==============================] - 0s 93us/step - loss: 222.4357 - val_loss: 228.2937\n",
      "Epoch 472/1200\n",
      "3477/3477 [==============================] - 0s 89us/step - loss: 220.2559 - val_loss: 226.3705\n",
      "Epoch 473/1200\n",
      "3477/3477 [==============================] - 0s 82us/step - loss: 218.1443 - val_loss: 224.5243\n",
      "Epoch 474/1200\n",
      "3477/3477 [==============================] - 0s 80us/step - loss: 216.0981 - val_loss: 222.7603\n",
      "Epoch 475/1200\n",
      "3477/3477 [==============================] - 0s 86us/step - loss: 214.1158 - val_loss: 221.0083\n",
      "Epoch 476/1200\n",
      "3477/3477 [==============================] - 0s 97us/step - loss: 212.2069 - val_loss: 219.3478\n",
      "Epoch 477/1200\n",
      "3477/3477 [==============================] - 0s 98us/step - loss: 210.3647 - val_loss: 217.7723\n",
      "Epoch 478/1200\n",
      "3477/3477 [==============================] - 0s 102us/step - loss: 208.5843 - val_loss: 216.2405\n",
      "Epoch 479/1200\n",
      "3477/3477 [==============================] - 0s 99us/step - loss: 206.8626 - val_loss: 214.7620\n",
      "Epoch 480/1200\n",
      "3477/3477 [==============================] - 0s 93us/step - loss: 205.2037 - val_loss: 213.3643\n",
      "Epoch 481/1200\n",
      "3477/3477 [==============================] - 0s 88us/step - loss: 203.6157 - val_loss: 212.0228\n",
      "Epoch 482/1200\n",
      "3477/3477 [==============================] - 0s 81us/step - loss: 202.0871 - val_loss: 210.7361\n",
      "Epoch 483/1200\n",
      "3477/3477 [==============================] - 0s 84us/step - loss: 200.6205 - val_loss: 209.4948\n",
      "Epoch 484/1200\n",
      "3477/3477 [==============================] - 0s 87us/step - loss: 199.2135 - val_loss: 208.3441\n",
      "Epoch 485/1200\n",
      "3477/3477 [==============================] - 0s 95us/step - loss: 197.8675 - val_loss: 207.2268\n",
      "Epoch 486/1200\n",
      "3477/3477 [==============================] - 0s 85us/step - loss: 196.5853 - val_loss: 206.1676\n",
      "Epoch 487/1200\n",
      "3477/3477 [==============================] - 0s 81us/step - loss: 195.3492 - val_loss: 205.1823\n",
      "Epoch 488/1200\n",
      "3477/3477 [==============================] - 0s 80us/step - loss: 194.1740 - val_loss: 204.2242\n",
      "Epoch 489/1200\n",
      "3477/3477 [==============================] - 0s 86us/step - loss: 193.0537 - val_loss: 203.3330\n",
      "Epoch 490/1200\n",
      "3477/3477 [==============================] - 0s 81us/step - loss: 191.9910 - val_loss: 202.4800\n",
      "Epoch 491/1200\n",
      "3477/3477 [==============================] - 0s 83us/step - loss: 190.9781 - val_loss: 201.6938\n",
      "Epoch 492/1200\n",
      "3477/3477 [==============================] - 0s 83us/step - loss: 190.0227 - val_loss: 200.9428\n",
      "Epoch 493/1200\n",
      "3477/3477 [==============================] - 0s 82us/step - loss: 189.1160 - val_loss: 200.2460\n",
      "Epoch 494/1200\n",
      "3477/3477 [==============================] - 0s 85us/step - loss: 188.2613 - val_loss: 199.6103\n",
      "Epoch 495/1200\n",
      "3477/3477 [==============================] - 0s 84us/step - loss: 187.4546 - val_loss: 198.9986\n",
      "Epoch 496/1200\n",
      "3477/3477 [==============================] - 0s 81us/step - loss: 186.6956 - val_loss: 198.4372\n",
      "Epoch 497/1200\n",
      "3477/3477 [==============================] - 0s 79us/step - loss: 185.9773 - val_loss: 197.9127\n",
      "Epoch 498/1200\n",
      "3477/3477 [==============================] - 0s 81us/step - loss: 185.3027 - val_loss: 197.4558\n",
      "Epoch 499/1200\n",
      "3477/3477 [==============================] - 0s 82us/step - loss: 184.6698 - val_loss: 196.9922\n",
      "Epoch 500/1200\n",
      "3477/3477 [==============================] - 0s 82us/step - loss: 184.0755 - val_loss: 196.5810\n",
      "Epoch 501/1200\n",
      "3477/3477 [==============================] - 0s 81us/step - loss: 183.5285 - val_loss: 196.2123\n",
      "Epoch 502/1200\n",
      "3477/3477 [==============================] - 0s 106us/step - loss: 183.0219 - val_loss: 195.8690\n",
      "Epoch 503/1200\n",
      "3477/3477 [==============================] - 0s 80us/step - loss: 182.5481 - val_loss: 195.5709\n",
      "Epoch 504/1200\n",
      "3477/3477 [==============================] - 0s 81us/step - loss: 182.1120 - val_loss: 195.2911\n",
      "Epoch 505/1200\n",
      "3477/3477 [==============================] - 0s 83us/step - loss: 181.7058 - val_loss: 195.0506\n",
      "Epoch 506/1200\n",
      "3477/3477 [==============================] - 0s 80us/step - loss: 181.3336 - val_loss: 194.8344\n",
      "Epoch 507/1200\n",
      "3477/3477 [==============================] - 0s 97us/step - loss: 180.9950 - val_loss: 194.6378\n",
      "Epoch 508/1200\n",
      "3477/3477 [==============================] - 0s 80us/step - loss: 180.6816 - val_loss: 194.4703\n",
      "Epoch 509/1200\n",
      "3477/3477 [==============================] - 0s 98us/step - loss: 180.3985 - val_loss: 194.3263\n",
      "Epoch 510/1200\n",
      "3477/3477 [==============================] - 0s 78us/step - loss: 180.1403 - val_loss: 194.2003\n",
      "Epoch 511/1200\n",
      "3477/3477 [==============================] - 0s 100us/step - loss: 179.9123 - val_loss: 194.0872\n",
      "Epoch 512/1200\n",
      "3477/3477 [==============================] - 0s 82us/step - loss: 179.6982 - val_loss: 194.0057\n",
      "Epoch 513/1200\n",
      "3477/3477 [==============================] - 0s 80us/step - loss: 179.5071 - val_loss: 193.9349\n",
      "Epoch 514/1200\n",
      "3477/3477 [==============================] - 0s 81us/step - loss: 179.3391 - val_loss: 193.8745\n",
      "Epoch 515/1200\n",
      "3477/3477 [==============================] - 0s 99us/step - loss: 179.1847 - val_loss: 193.8282\n",
      "Epoch 516/1200\n",
      "3477/3477 [==============================] - 0s 98us/step - loss: 179.0490 - val_loss: 193.7906\n",
      "Epoch 517/1200\n",
      "3477/3477 [==============================] - 0s 98us/step - loss: 178.9314 - val_loss: 193.7649\n",
      "Epoch 518/1200\n",
      "3477/3477 [==============================] - 0s 88us/step - loss: 178.8262 - val_loss: 193.7504\n",
      "Epoch 519/1200\n",
      "3477/3477 [==============================] - 0s 93us/step - loss: 178.7315 - val_loss: 193.7409\n",
      "Epoch 520/1200\n",
      "3477/3477 [==============================] - 0s 101us/step - loss: 178.6481 - val_loss: 193.7389\n",
      "Epoch 521/1200\n",
      "3477/3477 [==============================] - 0s 84us/step - loss: 178.5817 - val_loss: 193.7414\n",
      "Epoch 522/1200\n",
      "3477/3477 [==============================] - 0s 84us/step - loss: 178.5128 - val_loss: 193.7496\n",
      "Epoch 523/1200\n",
      "3477/3477 [==============================] - 0s 86us/step - loss: 178.4585 - val_loss: 193.7599\n",
      "Epoch 524/1200\n",
      "3477/3477 [==============================] - 0s 85us/step - loss: 178.4146 - val_loss: 193.7746\n",
      "Epoch 525/1200\n",
      "3477/3477 [==============================] - 0s 81us/step - loss: 178.3750 - val_loss: 193.7875\n",
      "Epoch 526/1200\n",
      "3477/3477 [==============================] - 0s 81us/step - loss: 178.3376 - val_loss: 193.8074\n",
      "Epoch 527/1200\n",
      "3477/3477 [==============================] - 0s 80us/step - loss: 178.3079 - val_loss: 193.8296\n",
      "Epoch 528/1200\n",
      "3477/3477 [==============================] - 0s 84us/step - loss: 178.2826 - val_loss: 193.8479\n",
      "Epoch 529/1200\n",
      "3477/3477 [==============================] - 0s 83us/step - loss: 178.2611 - val_loss: 193.8690\n",
      "Epoch 530/1200\n",
      "3477/3477 [==============================] - 0s 81us/step - loss: 178.2426 - val_loss: 193.8866\n",
      "Epoch 531/1200\n",
      "3477/3477 [==============================] - 0s 81us/step - loss: 178.2248 - val_loss: 193.9016\n",
      "Epoch 532/1200\n",
      "3477/3477 [==============================] - 0s 83us/step - loss: 178.2102 - val_loss: 193.9274\n",
      "Epoch 533/1200\n",
      "3477/3477 [==============================] - 0s 83us/step - loss: 178.1996 - val_loss: 193.9460\n",
      "Epoch 534/1200\n",
      "3477/3477 [==============================] - 0s 81us/step - loss: 178.1908 - val_loss: 193.9635\n",
      "Epoch 535/1200\n",
      "3477/3477 [==============================] - 0s 85us/step - loss: 178.1805 - val_loss: 193.9829\n",
      "Epoch 536/1200\n",
      "3477/3477 [==============================] - 0s 84us/step - loss: 178.1763 - val_loss: 193.9950\n",
      "Epoch 537/1200\n",
      "3477/3477 [==============================] - 0s 79us/step - loss: 178.1725 - val_loss: 194.0225\n",
      "Epoch 538/1200\n",
      "3477/3477 [==============================] - 0s 81us/step - loss: 178.1615 - val_loss: 194.0273\n",
      "Epoch 539/1200\n",
      "3477/3477 [==============================] - 0s 82us/step - loss: 178.1587 - val_loss: 194.0411\n",
      "Epoch 540/1200\n",
      "3477/3477 [==============================] - 0s 81us/step - loss: 178.1561 - val_loss: 194.0567\n",
      "Epoch 541/1200\n",
      "3477/3477 [==============================] - 0s 81us/step - loss: 178.1538 - val_loss: 194.0619\n",
      "Epoch 542/1200\n",
      "3477/3477 [==============================] - 0s 81us/step - loss: 178.1513 - val_loss: 194.0744\n",
      "Epoch 543/1200\n",
      "3477/3477 [==============================] - 0s 80us/step - loss: 178.1487 - val_loss: 194.0956\n",
      "Epoch 544/1200\n",
      "3477/3477 [==============================] - 0s 82us/step - loss: 178.1459 - val_loss: 194.1038\n",
      "Epoch 545/1200\n",
      "3477/3477 [==============================] - 0s 79us/step - loss: 178.1498 - val_loss: 194.1000\n",
      "Epoch 546/1200\n",
      "3477/3477 [==============================] - 0s 97us/step - loss: 178.1424 - val_loss: 194.1149\n",
      "Epoch 547/1200\n",
      "3477/3477 [==============================] - 0s 92us/step - loss: 178.1462 - val_loss: 194.1414\n",
      "Epoch 548/1200\n",
      "3477/3477 [==============================] - 0s 78us/step - loss: 178.1430 - val_loss: 194.1342\n",
      "Epoch 549/1200\n",
      "3477/3477 [==============================] - 0s 81us/step - loss: 178.1417 - val_loss: 194.1481\n",
      "Epoch 550/1200\n",
      "3477/3477 [==============================] - 0s 79us/step - loss: 178.1395 - val_loss: 194.1435\n",
      "Epoch 551/1200\n",
      "3477/3477 [==============================] - 0s 86us/step - loss: 178.1394 - val_loss: 194.1528\n",
      "Epoch 552/1200\n",
      "3477/3477 [==============================] - 0s 81us/step - loss: 178.1407 - val_loss: 194.1527\n",
      "Epoch 553/1200\n",
      "3477/3477 [==============================] - 0s 82us/step - loss: 178.1415 - val_loss: 194.1588\n",
      "Epoch 554/1200\n",
      "3477/3477 [==============================] - 0s 81us/step - loss: 178.1388 - val_loss: 194.1674\n",
      "Epoch 555/1200\n",
      "3477/3477 [==============================] - 0s 82us/step - loss: 178.1414 - val_loss: 194.1727\n",
      "Epoch 556/1200\n",
      "3477/3477 [==============================] - 0s 83us/step - loss: 178.1382 - val_loss: 194.1710\n",
      "Epoch 557/1200\n",
      "3477/3477 [==============================] - 0s 79us/step - loss: 178.1387 - val_loss: 194.1768\n",
      "Epoch 558/1200\n",
      "3477/3477 [==============================] - 0s 79us/step - loss: 178.1376 - val_loss: 194.1900\n",
      "Epoch 559/1200\n",
      "3477/3477 [==============================] - 0s 81us/step - loss: 178.1386 - val_loss: 194.1987\n",
      "Epoch 560/1200\n",
      "3477/3477 [==============================] - 0s 82us/step - loss: 178.1431 - val_loss: 194.1828\n",
      "Epoch 561/1200\n",
      "3477/3477 [==============================] - 0s 80us/step - loss: 178.1374 - val_loss: 194.1878\n",
      "Epoch 562/1200\n",
      "3477/3477 [==============================] - 0s 79us/step - loss: 178.1385 - val_loss: 194.2030\n",
      "Epoch 563/1200\n",
      "3477/3477 [==============================] - 0s 81us/step - loss: 178.1386 - val_loss: 194.1959\n",
      "Epoch 564/1200\n",
      "3477/3477 [==============================] - 0s 81us/step - loss: 178.1385 - val_loss: 194.2021\n",
      "Epoch 565/1200\n",
      "3477/3477 [==============================] - 0s 81us/step - loss: 178.1379 - val_loss: 194.1978\n",
      "Epoch 566/1200\n",
      "3477/3477 [==============================] - 0s 84us/step - loss: 178.1372 - val_loss: 194.1983\n",
      "Epoch 567/1200\n",
      "3477/3477 [==============================] - 0s 82us/step - loss: 178.1367 - val_loss: 194.2040\n",
      "Epoch 568/1200\n",
      "3477/3477 [==============================] - 0s 79us/step - loss: 178.1376 - val_loss: 194.2060\n",
      "Epoch 569/1200\n",
      "3477/3477 [==============================] - 0s 79us/step - loss: 178.1437 - val_loss: 194.2000\n",
      "Epoch 570/1200\n",
      "3477/3477 [==============================] - 0s 98us/step - loss: 178.1379 - val_loss: 194.2182\n",
      "Epoch 571/1200\n",
      "3477/3477 [==============================] - 0s 103us/step - loss: 178.1386 - val_loss: 194.2203\n",
      "Epoch 572/1200\n",
      "3477/3477 [==============================] - 0s 96us/step - loss: 178.1391 - val_loss: 194.2088\n",
      "Epoch 573/1200\n",
      "3477/3477 [==============================] - 0s 85us/step - loss: 178.1386 - val_loss: 194.2148\n",
      "Epoch 574/1200\n",
      "3477/3477 [==============================] - 0s 112us/step - loss: 178.1387 - val_loss: 194.2058\n",
      "Epoch 575/1200\n",
      "3477/3477 [==============================] - 0s 92us/step - loss: 178.1370 - val_loss: 194.2117\n",
      "Epoch 576/1200\n",
      "3477/3477 [==============================] - 0s 83us/step - loss: 178.1388 - val_loss: 194.2133\n",
      "Epoch 577/1200\n",
      "3477/3477 [==============================] - 0s 82us/step - loss: 178.1391 - val_loss: 194.2071\n",
      "Epoch 578/1200\n",
      "3477/3477 [==============================] - 0s 82us/step - loss: 178.1395 - val_loss: 194.2130\n",
      "Epoch 579/1200\n",
      "3477/3477 [==============================] - 0s 83us/step - loss: 178.1377 - val_loss: 194.2083\n",
      "Epoch 580/1200\n",
      "3477/3477 [==============================] - 0s 85us/step - loss: 178.1406 - val_loss: 194.2147\n",
      "Epoch 581/1200\n",
      "3477/3477 [==============================] - 0s 80us/step - loss: 178.1389 - val_loss: 194.2111\n",
      "Epoch 582/1200\n",
      "3477/3477 [==============================] - 0s 81us/step - loss: 178.1372 - val_loss: 194.2153\n",
      "Epoch 583/1200\n",
      "3477/3477 [==============================] - 0s 81us/step - loss: 178.1383 - val_loss: 194.2179\n",
      "Epoch 584/1200\n",
      "3477/3477 [==============================] - 0s 79us/step - loss: 178.1380 - val_loss: 194.2258\n",
      "Epoch 585/1200\n",
      "3477/3477 [==============================] - 0s 78us/step - loss: 178.1378 - val_loss: 194.2166\n",
      "Epoch 586/1200\n",
      "3477/3477 [==============================] - 0s 79us/step - loss: 178.1384 - val_loss: 194.2102\n",
      "Epoch 587/1200\n",
      "3477/3477 [==============================] - 0s 83us/step - loss: 178.1384 - val_loss: 194.2158\n",
      "Epoch 588/1200\n",
      "3477/3477 [==============================] - 0s 80us/step - loss: 178.1382 - val_loss: 194.2165\n",
      "Epoch 589/1200\n",
      "3477/3477 [==============================] - 0s 81us/step - loss: 178.1358 - val_loss: 194.2172\n",
      "Epoch 590/1200\n",
      "3477/3477 [==============================] - 0s 80us/step - loss: 178.1379 - val_loss: 194.2197\n",
      "Epoch 591/1200\n",
      "3477/3477 [==============================] - 0s 82us/step - loss: 178.1416 - val_loss: 194.2139\n",
      "Epoch 592/1200\n",
      "3477/3477 [==============================] - 0s 80us/step - loss: 178.1378 - val_loss: 194.2120\n",
      "Epoch 593/1200\n",
      "3477/3477 [==============================] - 0s 82us/step - loss: 178.1362 - val_loss: 194.2140\n",
      "Epoch 594/1200\n",
      "3477/3477 [==============================] - 0s 81us/step - loss: 178.1373 - val_loss: 194.2155\n",
      "Epoch 595/1200\n",
      "3477/3477 [==============================] - 0s 84us/step - loss: 178.1369 - val_loss: 194.2210\n",
      "Epoch 596/1200\n",
      "3477/3477 [==============================] - 0s 91us/step - loss: 178.1361 - val_loss: 194.2170\n",
      "Epoch 597/1200\n",
      "3477/3477 [==============================] - 0s 82us/step - loss: 178.1371 - val_loss: 194.2230\n",
      "Epoch 598/1200\n",
      "3477/3477 [==============================] - 0s 82us/step - loss: 178.1419 - val_loss: 194.2270\n",
      "Epoch 599/1200\n",
      "3477/3477 [==============================] - 0s 92us/step - loss: 178.1367 - val_loss: 194.2184\n",
      "Epoch 600/1200\n",
      "3477/3477 [==============================] - 0s 77us/step - loss: 178.1367 - val_loss: 194.2213\n",
      "Epoch 601/1200\n",
      "3477/3477 [==============================] - 0s 82us/step - loss: 178.1359 - val_loss: 194.2151\n",
      "Epoch 602/1200\n",
      "3477/3477 [==============================] - 0s 82us/step - loss: 178.1397 - val_loss: 194.2219\n",
      "Epoch 603/1200\n",
      "3477/3477 [==============================] - 0s 111us/step - loss: 178.1396 - val_loss: 194.2188\n",
      "Epoch 604/1200\n",
      "3477/3477 [==============================] - 0s 77us/step - loss: 178.1368 - val_loss: 194.2264\n",
      "Epoch 605/1200\n",
      "3477/3477 [==============================] - 0s 79us/step - loss: 178.1377 - val_loss: 194.2227\n",
      "Epoch 606/1200\n",
      "3477/3477 [==============================] - 0s 81us/step - loss: 178.1365 - val_loss: 194.2179\n",
      "Epoch 607/1200\n",
      "3477/3477 [==============================] - 0s 83us/step - loss: 178.1368 - val_loss: 194.2215\n",
      "Epoch 608/1200\n",
      "3477/3477 [==============================] - 0s 83us/step - loss: 178.1389 - val_loss: 194.2134\n",
      "Epoch 609/1200\n",
      "3477/3477 [==============================] - 0s 79us/step - loss: 178.1387 - val_loss: 194.2229\n",
      "Epoch 610/1200\n",
      "3477/3477 [==============================] - 0s 79us/step - loss: 178.1368 - val_loss: 194.2141\n",
      "Epoch 611/1200\n",
      "3477/3477 [==============================] - 0s 97us/step - loss: 178.1382 - val_loss: 194.2168\n",
      "Epoch 612/1200\n",
      "3477/3477 [==============================] - 0s 77us/step - loss: 178.1390 - val_loss: 194.2165\n",
      "Epoch 613/1200\n",
      "3477/3477 [==============================] - 0s 81us/step - loss: 178.1376 - val_loss: 194.2194\n",
      "Epoch 614/1200\n",
      "3477/3477 [==============================] - 0s 86us/step - loss: 178.1391 - val_loss: 194.2056\n",
      "Epoch 615/1200\n",
      "3477/3477 [==============================] - 0s 77us/step - loss: 178.1405 - val_loss: 194.2166\n",
      "Epoch 616/1200\n",
      "3477/3477 [==============================] - 0s 85us/step - loss: 178.1373 - val_loss: 194.2179\n",
      "Epoch 617/1200\n",
      "3477/3477 [==============================] - 0s 81us/step - loss: 178.1386 - val_loss: 194.2210\n",
      "Epoch 618/1200\n",
      "3477/3477 [==============================] - 0s 83us/step - loss: 178.1389 - val_loss: 194.2040\n",
      "Epoch 619/1200\n",
      "3477/3477 [==============================] - 0s 79us/step - loss: 178.1372 - val_loss: 194.2105\n",
      "Epoch 620/1200\n",
      "3477/3477 [==============================] - 0s 94us/step - loss: 178.1382 - val_loss: 194.2185\n",
      "Epoch 621/1200\n",
      "3477/3477 [==============================] - 0s 78us/step - loss: 178.1382 - val_loss: 194.2159\n",
      "Epoch 622/1200\n",
      "3477/3477 [==============================] - 0s 79us/step - loss: 178.1368 - val_loss: 194.2195\n",
      "Epoch 623/1200\n",
      "3477/3477 [==============================] - 0s 79us/step - loss: 178.1356 - val_loss: 194.2246\n",
      "Epoch 624/1200\n",
      "3477/3477 [==============================] - 0s 87us/step - loss: 178.1413 - val_loss: 194.2133\n",
      "Epoch 625/1200\n",
      "3477/3477 [==============================] - 0s 100us/step - loss: 178.1361 - val_loss: 194.2234\n",
      "Epoch 626/1200\n",
      "3477/3477 [==============================] - 0s 98us/step - loss: 178.1383 - val_loss: 194.2239\n",
      "Epoch 627/1200\n",
      "3477/3477 [==============================] - 0s 109us/step - loss: 178.1375 - val_loss: 194.2228\n",
      "Epoch 628/1200\n",
      "3477/3477 [==============================] - 0s 91us/step - loss: 178.1385 - val_loss: 194.2286\n",
      "Epoch 629/1200\n",
      "3477/3477 [==============================] - 0s 99us/step - loss: 178.1372 - val_loss: 194.2179\n",
      "Epoch 630/1200\n",
      "3477/3477 [==============================] - 0s 88us/step - loss: 178.1396 - val_loss: 194.2153\n",
      "Epoch 631/1200\n",
      "3477/3477 [==============================] - 0s 82us/step - loss: 178.1363 - val_loss: 194.2228\n",
      "Epoch 632/1200\n",
      "3477/3477 [==============================] - 0s 78us/step - loss: 178.1357 - val_loss: 194.2147\n",
      "Epoch 633/1200\n",
      "3477/3477 [==============================] - 0s 83us/step - loss: 178.1378 - val_loss: 194.2128\n",
      "Epoch 634/1200\n",
      "3477/3477 [==============================] - 0s 85us/step - loss: 178.1362 - val_loss: 194.2187\n",
      "Epoch 635/1200\n",
      "3477/3477 [==============================] - 0s 80us/step - loss: 178.1390 - val_loss: 194.2197\n",
      "Epoch 636/1200\n",
      "3477/3477 [==============================] - 0s 81us/step - loss: 178.1370 - val_loss: 194.2244\n",
      "Epoch 637/1200\n",
      "3477/3477 [==============================] - 0s 81us/step - loss: 178.1365 - val_loss: 194.2193\n",
      "Epoch 638/1200\n",
      "3477/3477 [==============================] - 0s 87us/step - loss: 178.1372 - val_loss: 194.2182\n",
      "Epoch 639/1200\n",
      "3477/3477 [==============================] - 0s 80us/step - loss: 178.1379 - val_loss: 194.2245\n",
      "Epoch 640/1200\n",
      "3477/3477 [==============================] - 0s 79us/step - loss: 178.1377 - val_loss: 194.2165\n",
      "Epoch 641/1200\n",
      "3477/3477 [==============================] - 0s 80us/step - loss: 178.1385 - val_loss: 194.2209\n",
      "Epoch 642/1200\n",
      "3477/3477 [==============================] - 0s 81us/step - loss: 178.1363 - val_loss: 194.2216\n",
      "Epoch 643/1200\n",
      "3477/3477 [==============================] - 0s 81us/step - loss: 178.1390 - val_loss: 194.2222\n",
      "Epoch 644/1200\n",
      "3477/3477 [==============================] - 0s 95us/step - loss: 178.1405 - val_loss: 194.2103\n",
      "Epoch 645/1200\n",
      "3477/3477 [==============================] - 0s 78us/step - loss: 178.1380 - val_loss: 194.2072\n",
      "Epoch 646/1200\n",
      "3477/3477 [==============================] - 0s 79us/step - loss: 178.1380 - val_loss: 194.2172\n",
      "Epoch 647/1200\n",
      "3477/3477 [==============================] - 0s 80us/step - loss: 178.1400 - val_loss: 194.2264\n",
      "Epoch 648/1200\n",
      "3477/3477 [==============================] - 0s 80us/step - loss: 178.1368 - val_loss: 194.2175\n",
      "Epoch 649/1200\n",
      "3477/3477 [==============================] - 0s 82us/step - loss: 178.1382 - val_loss: 194.2073\n",
      "Epoch 650/1200\n",
      "3477/3477 [==============================] - 0s 80us/step - loss: 178.1366 - val_loss: 194.2281\n",
      "Epoch 651/1200\n",
      "3477/3477 [==============================] - 0s 79us/step - loss: 178.1368 - val_loss: 194.2268\n",
      "Epoch 652/1200\n",
      "3477/3477 [==============================] - 0s 83us/step - loss: 178.1383 - val_loss: 194.2087\n",
      "Epoch 653/1200\n",
      "3477/3477 [==============================] - 0s 80us/step - loss: 178.1397 - val_loss: 194.2239\n",
      "Epoch 654/1200\n",
      "3477/3477 [==============================] - 0s 80us/step - loss: 178.1375 - val_loss: 194.2202\n",
      "Epoch 655/1200\n",
      "3477/3477 [==============================] - 0s 79us/step - loss: 178.1376 - val_loss: 194.2268\n",
      "Epoch 656/1200\n",
      "3477/3477 [==============================] - 0s 79us/step - loss: 178.1378 - val_loss: 194.2222\n",
      "Epoch 657/1200\n",
      "3477/3477 [==============================] - 0s 96us/step - loss: 178.1377 - val_loss: 194.2209\n",
      "Epoch 658/1200\n",
      "3477/3477 [==============================] - 0s 82us/step - loss: 178.1357 - val_loss: 194.2194\n",
      "Epoch 659/1200\n",
      "3477/3477 [==============================] - 0s 81us/step - loss: 178.1370 - val_loss: 194.2207\n",
      "Epoch 660/1200\n",
      "3477/3477 [==============================] - 0s 92us/step - loss: 178.1391 - val_loss: 194.2229\n",
      "Epoch 661/1200\n",
      "3477/3477 [==============================] - 0s 95us/step - loss: 178.1370 - val_loss: 194.2240\n",
      "Epoch 662/1200\n",
      "3477/3477 [==============================] - 0s 92us/step - loss: 178.1356 - val_loss: 194.2215\n",
      "Epoch 663/1200\n",
      "3477/3477 [==============================] - 0s 91us/step - loss: 178.1372 - val_loss: 194.2186\n",
      "Epoch 664/1200\n",
      "3477/3477 [==============================] - 0s 80us/step - loss: 91.7974 - val_loss: 64.0341\n",
      "Epoch 665/1200\n",
      "3477/3477 [==============================] - 0s 84us/step - loss: 62.7470 - val_loss: 59.9239\n",
      "Epoch 666/1200\n",
      "3477/3477 [==============================] - 0s 89us/step - loss: 59.0517 - val_loss: 56.6250\n",
      "Epoch 667/1200\n",
      "3477/3477 [==============================] - 0s 84us/step - loss: 55.9207 - val_loss: 53.7886\n",
      "Epoch 668/1200\n",
      "3477/3477 [==============================] - 0s 87us/step - loss: 53.1483 - val_loss: 51.2559\n",
      "Epoch 669/1200\n",
      "3477/3477 [==============================] - 0s 92us/step - loss: 50.6737 - val_loss: 48.9705\n",
      "Epoch 670/1200\n",
      "3477/3477 [==============================] - 0s 97us/step - loss: 48.4273 - val_loss: 46.8782\n",
      "Epoch 671/1200\n",
      "3477/3477 [==============================] - 0s 89us/step - loss: 46.3713 - val_loss: 44.9728\n",
      "Epoch 672/1200\n",
      "3477/3477 [==============================] - 0s 88us/step - loss: 44.4223 - val_loss: 43.1528\n",
      "Epoch 673/1200\n",
      "3477/3477 [==============================] - 0s 86us/step - loss: 42.6426 - val_loss: 41.4791\n",
      "Epoch 674/1200\n",
      "3477/3477 [==============================] - 0s 93us/step - loss: 40.9449 - val_loss: 39.8801\n",
      "Epoch 675/1200\n",
      "3477/3477 [==============================] - 0s 104us/step - loss: 39.3571 - val_loss: 38.4044\n",
      "Epoch 676/1200\n",
      "3477/3477 [==============================] - 0s 90us/step - loss: 37.8408 - val_loss: 37.0126\n",
      "Epoch 677/1200\n",
      "3477/3477 [==============================] - 0s 119us/step - loss: 36.3954 - val_loss: 35.5797\n",
      "Epoch 678/1200\n",
      "3477/3477 [==============================] - 0s 108us/step - loss: 35.0435 - val_loss: 34.2875\n",
      "Epoch 679/1200\n",
      "3477/3477 [==============================] - 0s 118us/step - loss: 33.7317 - val_loss: 33.0794\n",
      "Epoch 680/1200\n",
      "3477/3477 [==============================] - 0s 97us/step - loss: 32.4883 - val_loss: 31.9262\n",
      "Epoch 681/1200\n",
      "3477/3477 [==============================] - 0s 123us/step - loss: 31.3122 - val_loss: 30.7877\n",
      "Epoch 682/1200\n",
      "3477/3477 [==============================] - 0s 94us/step - loss: 30.1811 - val_loss: 29.7847\n",
      "Epoch 683/1200\n",
      "3477/3477 [==============================] - 0s 85us/step - loss: 29.0559 - val_loss: 28.6884\n",
      "Epoch 684/1200\n",
      "3477/3477 [==============================] - 0s 89us/step - loss: 28.0060 - val_loss: 27.6899\n",
      "Epoch 685/1200\n",
      "3477/3477 [==============================] - 0s 82us/step - loss: 26.9663 - val_loss: 26.6908\n",
      "Epoch 686/1200\n",
      "3477/3477 [==============================] - 0s 85us/step - loss: 26.0104 - val_loss: 25.7575\n",
      "Epoch 687/1200\n",
      "3477/3477 [==============================] - 0s 81us/step - loss: 25.0508 - val_loss: 24.8564\n",
      "Epoch 688/1200\n",
      "3477/3477 [==============================] - 0s 81us/step - loss: 24.1324 - val_loss: 24.0117\n",
      "Epoch 689/1200\n",
      "3477/3477 [==============================] - 0s 79us/step - loss: 23.2437 - val_loss: 23.1395\n",
      "Epoch 690/1200\n",
      "3477/3477 [==============================] - 0s 82us/step - loss: 22.4150 - val_loss: 22.3783\n",
      "Epoch 691/1200\n",
      "3477/3477 [==============================] - 0s 100us/step - loss: 21.5629 - val_loss: 21.5917\n",
      "Epoch 692/1200\n",
      "3477/3477 [==============================] - 0s 79us/step - loss: 20.7708 - val_loss: 20.8803\n",
      "Epoch 693/1200\n",
      "3477/3477 [==============================] - 0s 79us/step - loss: 19.9736 - val_loss: 20.0629\n",
      "Epoch 694/1200\n",
      "3477/3477 [==============================] - 0s 93us/step - loss: 19.2296 - val_loss: 19.3204\n",
      "Epoch 695/1200\n",
      "3477/3477 [==============================] - 0s 81us/step - loss: 18.5151 - val_loss: 18.6943\n",
      "Epoch 696/1200\n",
      "3477/3477 [==============================] - 0s 79us/step - loss: 17.7937 - val_loss: 17.9987\n",
      "Epoch 697/1200\n",
      "3477/3477 [==============================] - 0s 82us/step - loss: 17.1328 - val_loss: 17.4103\n",
      "Epoch 698/1200\n",
      "3477/3477 [==============================] - 0s 82us/step - loss: 16.4919 - val_loss: 16.7528\n",
      "Epoch 699/1200\n",
      "3477/3477 [==============================] - 0s 95us/step - loss: 15.8268 - val_loss: 16.1072\n",
      "Epoch 700/1200\n",
      "3477/3477 [==============================] - 0s 79us/step - loss: 15.1937 - val_loss: 15.5105\n",
      "Epoch 701/1200\n",
      "3477/3477 [==============================] - 0s 81us/step - loss: 14.5948 - val_loss: 15.0008\n",
      "Epoch 702/1200\n",
      "3477/3477 [==============================] - 0s 82us/step - loss: 14.0203 - val_loss: 14.3902\n",
      "Epoch 703/1200\n",
      "3477/3477 [==============================] - 0s 80us/step - loss: 13.4629 - val_loss: 13.9829\n",
      "Epoch 704/1200\n",
      "3477/3477 [==============================] - 0s 79us/step - loss: 12.9162 - val_loss: 13.3037\n",
      "Epoch 705/1200\n",
      "3477/3477 [==============================] - 0s 79us/step - loss: 12.4188 - val_loss: 12.8408\n",
      "Epoch 706/1200\n",
      "3477/3477 [==============================] - 0s 79us/step - loss: 11.8916 - val_loss: 12.2909\n",
      "Epoch 707/1200\n",
      "3477/3477 [==============================] - 0s 80us/step - loss: 11.3730 - val_loss: 11.8286\n",
      "Epoch 708/1200\n",
      "3477/3477 [==============================] - 0s 79us/step - loss: 10.9222 - val_loss: 11.4267\n",
      "Epoch 709/1200\n",
      "3477/3477 [==============================] - 0s 86us/step - loss: 10.4631 - val_loss: 10.9954\n",
      "Epoch 710/1200\n",
      "3477/3477 [==============================] - 0s 80us/step - loss: 10.0133 - val_loss: 10.5020\n",
      "Epoch 711/1200\n",
      "3477/3477 [==============================] - 0s 82us/step - loss: 9.5841 - val_loss: 10.0703\n",
      "Epoch 712/1200\n",
      "3477/3477 [==============================] - 0s 81us/step - loss: 9.1583 - val_loss: 9.6702\n",
      "Epoch 713/1200\n",
      "3477/3477 [==============================] - 0s 79us/step - loss: 8.7938 - val_loss: 9.3472\n",
      "Epoch 714/1200\n",
      "3477/3477 [==============================] - 0s 82us/step - loss: 8.3719 - val_loss: 8.8676\n",
      "Epoch 715/1200\n",
      "3477/3477 [==============================] - 0s 81us/step - loss: 8.0194 - val_loss: 8.5275\n",
      "Epoch 716/1200\n",
      "3477/3477 [==============================] - 0s 80us/step - loss: 7.6617 - val_loss: 8.1593\n",
      "Epoch 717/1200\n",
      "3477/3477 [==============================] - 0s 80us/step - loss: 7.3176 - val_loss: 7.7671\n",
      "Epoch 718/1200\n",
      "3477/3477 [==============================] - 0s 83us/step - loss: 6.9980 - val_loss: 7.5306\n",
      "Epoch 719/1200\n",
      "3477/3477 [==============================] - 0s 84us/step - loss: 6.6859 - val_loss: 7.1331\n",
      "Epoch 720/1200\n",
      "3477/3477 [==============================] - 0s 83us/step - loss: 6.3453 - val_loss: 6.8787\n",
      "Epoch 721/1200\n",
      "3477/3477 [==============================] - 0s 81us/step - loss: 6.0641 - val_loss: 6.5180\n",
      "Epoch 722/1200\n",
      "3477/3477 [==============================] - 0s 80us/step - loss: 5.8049 - val_loss: 6.5124\n",
      "Epoch 723/1200\n",
      "3477/3477 [==============================] - 0s 84us/step - loss: 5.5111 - val_loss: 5.9885\n",
      "Epoch 724/1200\n",
      "3477/3477 [==============================] - 0s 80us/step - loss: 5.2319 - val_loss: 5.7326\n",
      "Epoch 725/1200\n",
      "3477/3477 [==============================] - 0s 79us/step - loss: 5.0303 - val_loss: 5.4772\n",
      "Epoch 726/1200\n",
      "3477/3477 [==============================] - 0s 78us/step - loss: 4.7682 - val_loss: 5.2833\n",
      "Epoch 727/1200\n",
      "3477/3477 [==============================] - 0s 80us/step - loss: 4.5147 - val_loss: 5.1058\n",
      "Epoch 728/1200\n",
      "3477/3477 [==============================] - 0s 81us/step - loss: 4.3376 - val_loss: 4.7807\n",
      "Epoch 729/1200\n",
      "3477/3477 [==============================] - 0s 78us/step - loss: 4.1126 - val_loss: 4.5703\n",
      "Epoch 730/1200\n",
      "3477/3477 [==============================] - 0s 80us/step - loss: 3.9268 - val_loss: 4.3640\n",
      "Epoch 731/1200\n",
      "3477/3477 [==============================] - 0s 91us/step - loss: 3.7169 - val_loss: 4.1719\n",
      "Epoch 732/1200\n",
      "3477/3477 [==============================] - 0s 94us/step - loss: 3.5437 - val_loss: 3.9831\n",
      "Epoch 733/1200\n",
      "3477/3477 [==============================] - 0s 100us/step - loss: 3.3876 - val_loss: 3.8274\n",
      "Epoch 734/1200\n",
      "3477/3477 [==============================] - 0s 92us/step - loss: 3.2361 - val_loss: 3.6378\n",
      "Epoch 735/1200\n",
      "3477/3477 [==============================] - 0s 87us/step - loss: 3.0804 - val_loss: 3.4972\n",
      "Epoch 736/1200\n",
      "3477/3477 [==============================] - 0s 96us/step - loss: 2.9449 - val_loss: 3.3361\n",
      "Epoch 737/1200\n",
      "3477/3477 [==============================] - 0s 91us/step - loss: 2.7928 - val_loss: 3.2352\n",
      "Epoch 738/1200\n",
      "3477/3477 [==============================] - 0s 84us/step - loss: 2.6861 - val_loss: 3.0462\n",
      "Epoch 739/1200\n",
      "3477/3477 [==============================] - 0s 86us/step - loss: 2.5616 - val_loss: 2.9129\n",
      "Epoch 740/1200\n",
      "3477/3477 [==============================] - 0s 85us/step - loss: 2.4431 - val_loss: 2.8549\n",
      "Epoch 741/1200\n",
      "3477/3477 [==============================] - 0s 79us/step - loss: 2.3382 - val_loss: 2.7022\n",
      "Epoch 742/1200\n",
      "3477/3477 [==============================] - 0s 82us/step - loss: 2.2425 - val_loss: 2.6334\n",
      "Epoch 743/1200\n",
      "3477/3477 [==============================] - 0s 79us/step - loss: 2.1315 - val_loss: 2.4971\n",
      "Epoch 744/1200\n",
      "3477/3477 [==============================] - 0s 100us/step - loss: 2.0664 - val_loss: 2.4414\n",
      "Epoch 745/1200\n",
      "3477/3477 [==============================] - 0s 78us/step - loss: 1.9589 - val_loss: 2.3240\n",
      "Epoch 746/1200\n",
      "3477/3477 [==============================] - 0s 78us/step - loss: 1.9333 - val_loss: 2.2489\n",
      "Epoch 747/1200\n",
      "3477/3477 [==============================] - 0s 81us/step - loss: 1.8029 - val_loss: 2.1883\n",
      "Epoch 748/1200\n",
      "3477/3477 [==============================] - 0s 81us/step - loss: 1.7312 - val_loss: 2.0487\n",
      "Epoch 749/1200\n",
      "3477/3477 [==============================] - 0s 80us/step - loss: 1.6669 - val_loss: 1.9929\n",
      "Epoch 750/1200\n",
      "3477/3477 [==============================] - 0s 80us/step - loss: 1.5847 - val_loss: 1.9406\n",
      "Epoch 751/1200\n",
      "3477/3477 [==============================] - 0s 81us/step - loss: 1.5330 - val_loss: 1.7991\n",
      "Epoch 752/1200\n",
      "3477/3477 [==============================] - 0s 80us/step - loss: 1.4852 - val_loss: 1.7866\n",
      "Epoch 753/1200\n",
      "3477/3477 [==============================] - 0s 79us/step - loss: 1.4270 - val_loss: 1.7635\n",
      "Epoch 754/1200\n",
      "3477/3477 [==============================] - 0s 82us/step - loss: 1.3615 - val_loss: 1.6894\n",
      "Epoch 755/1200\n",
      "3477/3477 [==============================] - 0s 78us/step - loss: 1.3124 - val_loss: 1.5806\n",
      "Epoch 756/1200\n",
      "3477/3477 [==============================] - 0s 83us/step - loss: 1.2948 - val_loss: 1.5631\n",
      "Epoch 757/1200\n",
      "3477/3477 [==============================] - 0s 80us/step - loss: 1.2526 - val_loss: 1.4934\n",
      "Epoch 758/1200\n",
      "3477/3477 [==============================] - 0s 80us/step - loss: 1.1940 - val_loss: 1.5535\n",
      "Epoch 759/1200\n",
      "3477/3477 [==============================] - 0s 79us/step - loss: 1.1825 - val_loss: 1.4576\n",
      "Epoch 760/1200\n",
      "3477/3477 [==============================] - 0s 81us/step - loss: 1.1317 - val_loss: 1.3867\n",
      "Epoch 761/1200\n",
      "3477/3477 [==============================] - 0s 81us/step - loss: 1.0783 - val_loss: 1.3204\n",
      "Epoch 762/1200\n",
      "3477/3477 [==============================] - 0s 81us/step - loss: 1.0792 - val_loss: 1.2968\n",
      "Epoch 763/1200\n",
      "3477/3477 [==============================] - 0s 78us/step - loss: 1.0061 - val_loss: 1.2927\n",
      "Epoch 764/1200\n",
      "3477/3477 [==============================] - 0s 79us/step - loss: 1.0154 - val_loss: 1.2313\n",
      "Epoch 765/1200\n",
      "3477/3477 [==============================] - 0s 80us/step - loss: 0.9729 - val_loss: 1.2703\n",
      "Epoch 766/1200\n",
      "3477/3477 [==============================] - 0s 89us/step - loss: 0.9388 - val_loss: 1.1883\n",
      "Epoch 767/1200\n",
      "3477/3477 [==============================] - 0s 79us/step - loss: 0.9064 - val_loss: 1.1481\n",
      "Epoch 768/1200\n",
      "3477/3477 [==============================] - 0s 78us/step - loss: 0.8874 - val_loss: 1.2545\n",
      "Epoch 769/1200\n",
      "3477/3477 [==============================] - 0s 80us/step - loss: 0.9284 - val_loss: 1.2276\n",
      "Epoch 770/1200\n",
      "3477/3477 [==============================] - 0s 79us/step - loss: 0.8600 - val_loss: 1.1293\n",
      "Epoch 771/1200\n",
      "3477/3477 [==============================] - 0s 81us/step - loss: 0.8389 - val_loss: 1.0926\n",
      "Epoch 772/1200\n",
      "3477/3477 [==============================] - 0s 81us/step - loss: 0.8153 - val_loss: 1.0780\n",
      "Epoch 773/1200\n",
      "3477/3477 [==============================] - 0s 81us/step - loss: 0.8067 - val_loss: 1.0511\n",
      "Epoch 774/1200\n",
      "3477/3477 [==============================] - 0s 80us/step - loss: 0.7767 - val_loss: 1.0778\n",
      "Epoch 775/1200\n",
      "3477/3477 [==============================] - 0s 79us/step - loss: 0.7561 - val_loss: 0.9718\n",
      "Epoch 776/1200\n",
      "3477/3477 [==============================] - 0s 83us/step - loss: 0.7463 - val_loss: 0.9576\n",
      "Epoch 777/1200\n",
      "3477/3477 [==============================] - 0s 85us/step - loss: 0.7210 - val_loss: 0.9395\n",
      "Epoch 778/1200\n",
      "3477/3477 [==============================] - 0s 78us/step - loss: 0.7072 - val_loss: 0.9260\n",
      "Epoch 779/1200\n",
      "3477/3477 [==============================] - 0s 78us/step - loss: 0.7152 - val_loss: 0.9249\n",
      "Epoch 780/1200\n",
      "3477/3477 [==============================] - 0s 82us/step - loss: 0.7270 - val_loss: 0.8674\n",
      "Epoch 781/1200\n",
      "3477/3477 [==============================] - 0s 80us/step - loss: 0.6693 - val_loss: 0.8717\n",
      "Epoch 782/1200\n",
      "3477/3477 [==============================] - 0s 83us/step - loss: 0.6537 - val_loss: 0.8345\n",
      "Epoch 783/1200\n",
      "3477/3477 [==============================] - 0s 81us/step - loss: 0.6505 - val_loss: 0.8406\n",
      "Epoch 784/1200\n",
      "3477/3477 [==============================] - 0s 81us/step - loss: 0.6377 - val_loss: 0.8299\n",
      "Epoch 785/1200\n",
      "3477/3477 [==============================] - 0s 83us/step - loss: 0.6193 - val_loss: 0.8742\n",
      "Epoch 786/1200\n",
      "3477/3477 [==============================] - 0s 81us/step - loss: 0.6306 - val_loss: 0.8017\n",
      "Epoch 787/1200\n",
      "3477/3477 [==============================] - 0s 102us/step - loss: 0.5937 - val_loss: 0.7827\n",
      "Epoch 788/1200\n",
      "3477/3477 [==============================] - 0s 96us/step - loss: 0.6036 - val_loss: 0.7700\n",
      "Epoch 789/1200\n",
      "3477/3477 [==============================] - 0s 98us/step - loss: 0.5857 - val_loss: 0.8033\n",
      "Epoch 790/1200\n",
      "3477/3477 [==============================] - 0s 82us/step - loss: 0.5708 - val_loss: 0.7508\n",
      "Epoch 791/1200\n",
      "3477/3477 [==============================] - 0s 104us/step - loss: 0.5737 - val_loss: 0.7682\n",
      "Epoch 792/1200\n",
      "3477/3477 [==============================] - 0s 90us/step - loss: 0.5648 - val_loss: 0.7349\n",
      "Epoch 793/1200\n",
      "3477/3477 [==============================] - 0s 88us/step - loss: 0.5588 - val_loss: 0.7401\n",
      "Epoch 794/1200\n",
      "3477/3477 [==============================] - 0s 83us/step - loss: 0.5519 - val_loss: 0.7346\n",
      "Epoch 795/1200\n",
      "3477/3477 [==============================] - 0s 87us/step - loss: 0.5484 - val_loss: 0.6926\n",
      "Epoch 796/1200\n",
      "3477/3477 [==============================] - 0s 80us/step - loss: 0.5335 - val_loss: 0.7433\n",
      "Epoch 797/1200\n",
      "3477/3477 [==============================] - 0s 80us/step - loss: 0.5489 - val_loss: 0.7110\n",
      "Epoch 798/1200\n",
      "3477/3477 [==============================] - 0s 83us/step - loss: 0.5650 - val_loss: 0.9244\n",
      "Epoch 799/1200\n",
      "3477/3477 [==============================] - 0s 81us/step - loss: 0.5615 - val_loss: 0.7153\n",
      "Epoch 800/1200\n",
      "3477/3477 [==============================] - 0s 81us/step - loss: 0.5088 - val_loss: 0.6608\n",
      "Epoch 801/1200\n",
      "3477/3477 [==============================] - 0s 81us/step - loss: 0.5153 - val_loss: 0.6792\n",
      "Epoch 802/1200\n",
      "3477/3477 [==============================] - 0s 81us/step - loss: 0.5004 - val_loss: 0.6963\n",
      "Epoch 803/1200\n",
      "3477/3477 [==============================] - 0s 84us/step - loss: 0.5146 - val_loss: 0.6733\n",
      "Epoch 804/1200\n",
      "3477/3477 [==============================] - 0s 81us/step - loss: 0.5452 - val_loss: 0.7030\n",
      "Epoch 805/1200\n",
      "3477/3477 [==============================] - 0s 80us/step - loss: 0.5147 - val_loss: 0.6644\n",
      "Epoch 806/1200\n",
      "3477/3477 [==============================] - 0s 78us/step - loss: 0.5005 - val_loss: 0.6682\n",
      "Epoch 807/1200\n",
      "3477/3477 [==============================] - 0s 100us/step - loss: 0.5120 - val_loss: 0.6642\n",
      "Epoch 808/1200\n",
      "3477/3477 [==============================] - 0s 80us/step - loss: 0.5011 - val_loss: 0.6555\n",
      "Epoch 809/1200\n",
      "3477/3477 [==============================] - 0s 79us/step - loss: 0.4862 - val_loss: 0.6230\n",
      "Epoch 810/1200\n",
      "3477/3477 [==============================] - 0s 83us/step - loss: 0.4964 - val_loss: 0.6445\n",
      "Epoch 811/1200\n",
      "3477/3477 [==============================] - 0s 79us/step - loss: 0.5349 - val_loss: 0.6391\n",
      "Epoch 812/1200\n",
      "3477/3477 [==============================] - 0s 79us/step - loss: 0.4807 - val_loss: 0.6923\n",
      "Epoch 813/1200\n",
      "3477/3477 [==============================] - 0s 82us/step - loss: 0.4930 - val_loss: 0.6405\n",
      "Epoch 814/1200\n",
      "3477/3477 [==============================] - 0s 83us/step - loss: 0.4830 - val_loss: 0.6606\n",
      "Epoch 815/1200\n",
      "3477/3477 [==============================] - 0s 82us/step - loss: 0.4845 - val_loss: 0.6561\n",
      "Epoch 816/1200\n",
      "3477/3477 [==============================] - 0s 78us/step - loss: 0.4689 - val_loss: 0.6081\n",
      "Epoch 817/1200\n",
      "3477/3477 [==============================] - 0s 86us/step - loss: 0.4652 - val_loss: 0.6706\n",
      "Epoch 818/1200\n",
      "3477/3477 [==============================] - 0s 80us/step - loss: 0.4727 - val_loss: 0.6496\n",
      "Epoch 819/1200\n",
      "3477/3477 [==============================] - 0s 79us/step - loss: 0.4661 - val_loss: 0.6197\n",
      "Epoch 820/1200\n",
      "3477/3477 [==============================] - 0s 80us/step - loss: 0.4871 - val_loss: 0.6241\n",
      "Epoch 821/1200\n",
      "3477/3477 [==============================] - 0s 80us/step - loss: 0.4701 - val_loss: 0.6579\n",
      "Epoch 822/1200\n",
      "3477/3477 [==============================] - 0s 82us/step - loss: 0.4697 - val_loss: 0.6188\n",
      "Epoch 823/1200\n",
      "3477/3477 [==============================] - 0s 80us/step - loss: 0.4544 - val_loss: 0.6090\n",
      "Epoch 824/1200\n",
      "3477/3477 [==============================] - 0s 82us/step - loss: 0.4818 - val_loss: 0.7115\n",
      "Epoch 825/1200\n",
      "3477/3477 [==============================] - 0s 88us/step - loss: 0.4571 - val_loss: 0.6163\n",
      "Epoch 826/1200\n",
      "3477/3477 [==============================] - 0s 81us/step - loss: 0.4763 - val_loss: 0.5896\n",
      "Epoch 827/1200\n",
      "3477/3477 [==============================] - 0s 79us/step - loss: 0.4704 - val_loss: 0.6523\n",
      "Epoch 828/1200\n",
      "3477/3477 [==============================] - 0s 81us/step - loss: 0.4499 - val_loss: 0.6074\n",
      "Epoch 829/1200\n",
      "3477/3477 [==============================] - 0s 100us/step - loss: 0.4505 - val_loss: 0.6015\n",
      "Epoch 830/1200\n",
      "3477/3477 [==============================] - 0s 86us/step - loss: 0.4555 - val_loss: 0.6442\n",
      "Epoch 831/1200\n",
      "3477/3477 [==============================] - 0s 79us/step - loss: 0.4864 - val_loss: 0.6377\n",
      "Epoch 832/1200\n",
      "3477/3477 [==============================] - 0s 81us/step - loss: 0.4531 - val_loss: 0.5965\n",
      "Epoch 833/1200\n",
      "3477/3477 [==============================] - 0s 84us/step - loss: 0.4352 - val_loss: 0.5986\n",
      "Epoch 834/1200\n",
      "3477/3477 [==============================] - 0s 80us/step - loss: 0.4445 - val_loss: 0.6182\n",
      "Epoch 835/1200\n",
      "3477/3477 [==============================] - 0s 84us/step - loss: 0.4419 - val_loss: 0.5973\n",
      "Epoch 836/1200\n",
      "3477/3477 [==============================] - 0s 82us/step - loss: 0.4359 - val_loss: 0.6069\n",
      "Epoch 837/1200\n",
      "3477/3477 [==============================] - 0s 80us/step - loss: 0.4521 - val_loss: 0.5762\n",
      "Epoch 838/1200\n",
      "3477/3477 [==============================] - 0s 79us/step - loss: 0.4334 - val_loss: 0.6494\n",
      "Epoch 839/1200\n",
      "3477/3477 [==============================] - 0s 78us/step - loss: 0.4685 - val_loss: 0.5873\n",
      "Epoch 840/1200\n",
      "3477/3477 [==============================] - 0s 79us/step - loss: 0.4505 - val_loss: 0.5914\n",
      "Epoch 841/1200\n",
      "3477/3477 [==============================] - 0s 82us/step - loss: 0.4434 - val_loss: 0.5958\n",
      "Epoch 842/1200\n",
      "3477/3477 [==============================] - 0s 97us/step - loss: 0.4637 - val_loss: 0.5954\n",
      "Epoch 843/1200\n",
      "3477/3477 [==============================] - 0s 97us/step - loss: 0.4374 - val_loss: 0.5708\n",
      "Epoch 844/1200\n",
      "3477/3477 [==============================] - 0s 99us/step - loss: 0.4327 - val_loss: 0.5736\n",
      "Epoch 845/1200\n",
      "3477/3477 [==============================] - 1s 190us/step - loss: 0.4397 - val_loss: 0.5685\n",
      "Epoch 846/1200\n",
      "3477/3477 [==============================] - 0s 83us/step - loss: 0.4452 - val_loss: 0.5759\n",
      "Epoch 847/1200\n",
      "3477/3477 [==============================] - 0s 82us/step - loss: 0.4341 - val_loss: 0.5795\n",
      "Epoch 848/1200\n",
      "3477/3477 [==============================] - 0s 81us/step - loss: 0.4408 - val_loss: 0.5846\n",
      "Epoch 849/1200\n",
      "3477/3477 [==============================] - 0s 79us/step - loss: 0.4281 - val_loss: 0.5789\n",
      "Epoch 850/1200\n",
      "3477/3477 [==============================] - 0s 81us/step - loss: 0.4400 - val_loss: 0.5625\n",
      "Epoch 851/1200\n",
      "3477/3477 [==============================] - 0s 81us/step - loss: 0.4667 - val_loss: 0.5686\n",
      "Epoch 852/1200\n",
      "3477/3477 [==============================] - 0s 80us/step - loss: 0.4280 - val_loss: 0.5678\n",
      "Epoch 853/1200\n",
      "3477/3477 [==============================] - 0s 82us/step - loss: 0.4375 - val_loss: 0.6090\n",
      "Epoch 854/1200\n",
      "3477/3477 [==============================] - 0s 82us/step - loss: 0.4367 - val_loss: 0.5968\n",
      "Epoch 855/1200\n",
      "3477/3477 [==============================] - 0s 82us/step - loss: 0.4548 - val_loss: 0.5800\n",
      "Epoch 856/1200\n",
      "3477/3477 [==============================] - 0s 79us/step - loss: 0.4339 - val_loss: 0.5757\n",
      "Epoch 857/1200\n",
      "3477/3477 [==============================] - 0s 80us/step - loss: 0.4306 - val_loss: 0.5721\n",
      "Epoch 858/1200\n",
      "3477/3477 [==============================] - 0s 80us/step - loss: 0.4288 - val_loss: 0.5699\n",
      "Epoch 859/1200\n",
      "3477/3477 [==============================] - 0s 79us/step - loss: 0.4322 - val_loss: 0.5938\n",
      "Epoch 860/1200\n",
      "3477/3477 [==============================] - 0s 79us/step - loss: 0.4537 - val_loss: 0.5874\n",
      "Epoch 861/1200\n",
      "3477/3477 [==============================] - 0s 80us/step - loss: 0.4295 - val_loss: 0.5677\n",
      "Epoch 862/1200\n",
      "3477/3477 [==============================] - 0s 78us/step - loss: 0.4380 - val_loss: 0.6077\n",
      "Epoch 863/1200\n",
      "3477/3477 [==============================] - 0s 80us/step - loss: 0.4346 - val_loss: 0.5587\n",
      "Epoch 864/1200\n",
      "3477/3477 [==============================] - 0s 81us/step - loss: 0.4366 - val_loss: 0.6184\n",
      "Epoch 865/1200\n",
      "3477/3477 [==============================] - 0s 82us/step - loss: 0.4305 - val_loss: 0.5716\n",
      "Epoch 866/1200\n",
      "3477/3477 [==============================] - 0s 81us/step - loss: 0.4177 - val_loss: 0.5778\n",
      "Epoch 867/1200\n",
      "3477/3477 [==============================] - 0s 79us/step - loss: 0.4245 - val_loss: 0.5504\n",
      "Epoch 868/1200\n",
      "3477/3477 [==============================] - 0s 79us/step - loss: 0.4444 - val_loss: 0.5780\n",
      "Epoch 869/1200\n",
      "3477/3477 [==============================] - 0s 80us/step - loss: 0.4345 - val_loss: 0.5554\n",
      "Epoch 870/1200\n",
      "3477/3477 [==============================] - 0s 79us/step - loss: 0.4328 - val_loss: 0.5844\n",
      "Epoch 871/1200\n",
      "3477/3477 [==============================] - 0s 78us/step - loss: 0.4319 - val_loss: 0.6311\n",
      "Epoch 872/1200\n",
      "3477/3477 [==============================] - 0s 79us/step - loss: 0.4436 - val_loss: 0.5743\n",
      "Epoch 873/1200\n",
      "3477/3477 [==============================] - 0s 79us/step - loss: 0.4177 - val_loss: 0.5627\n",
      "Epoch 874/1200\n",
      "3477/3477 [==============================] - 0s 78us/step - loss: 0.4193 - val_loss: 0.5702\n",
      "Epoch 875/1200\n",
      "3477/3477 [==============================] - 0s 83us/step - loss: 0.4345 - val_loss: 0.5974\n",
      "Epoch 876/1200\n",
      "3477/3477 [==============================] - 0s 102us/step - loss: 0.4245 - val_loss: 0.5969\n",
      "Epoch 877/1200\n",
      "3477/3477 [==============================] - 0s 82us/step - loss: 0.4164 - val_loss: 0.5852\n",
      "Epoch 878/1200\n",
      "3477/3477 [==============================] - 0s 78us/step - loss: 0.4293 - val_loss: 0.5599\n",
      "Epoch 879/1200\n",
      "3477/3477 [==============================] - 0s 84us/step - loss: 0.4332 - val_loss: 0.5672\n",
      "Epoch 880/1200\n",
      "3477/3477 [==============================] - 0s 77us/step - loss: 0.4167 - val_loss: 0.5579\n",
      "Epoch 881/1200\n",
      "3477/3477 [==============================] - 0s 80us/step - loss: 0.4350 - val_loss: 0.6100\n",
      "Epoch 882/1200\n",
      "3477/3477 [==============================] - 0s 79us/step - loss: 0.4187 - val_loss: 0.5811\n",
      "Epoch 883/1200\n",
      "3477/3477 [==============================] - 0s 79us/step - loss: 0.4187 - val_loss: 0.5617\n",
      "Epoch 884/1200\n",
      "3477/3477 [==============================] - 0s 78us/step - loss: 0.4124 - val_loss: 0.5689\n",
      "Epoch 885/1200\n",
      "3477/3477 [==============================] - 0s 100us/step - loss: 0.4101 - val_loss: 0.5534\n",
      "Epoch 886/1200\n",
      "3477/3477 [==============================] - 0s 87us/step - loss: 0.4112 - val_loss: 0.5505\n",
      "Epoch 887/1200\n",
      "3477/3477 [==============================] - 0s 79us/step - loss: 0.4219 - val_loss: 0.5581\n",
      "Epoch 888/1200\n",
      "3477/3477 [==============================] - 0s 80us/step - loss: 0.4130 - val_loss: 0.5907\n",
      "Epoch 889/1200\n",
      "3477/3477 [==============================] - 0s 78us/step - loss: 0.4270 - val_loss: 0.6054\n",
      "Epoch 890/1200\n",
      "3477/3477 [==============================] - 0s 81us/step - loss: 0.4309 - val_loss: 0.5738\n",
      "Epoch 891/1200\n",
      "3477/3477 [==============================] - 0s 79us/step - loss: 0.4086 - val_loss: 0.5571\n",
      "Epoch 892/1200\n",
      "3477/3477 [==============================] - 0s 80us/step - loss: 0.4055 - val_loss: 0.5628\n",
      "Epoch 893/1200\n",
      "3477/3477 [==============================] - 0s 77us/step - loss: 0.4100 - val_loss: 0.5451\n",
      "Epoch 894/1200\n",
      "3477/3477 [==============================] - 0s 79us/step - loss: 0.4261 - val_loss: 0.6171\n",
      "Epoch 895/1200\n",
      "3477/3477 [==============================] - 0s 79us/step - loss: 0.4244 - val_loss: 0.6303\n",
      "Epoch 896/1200\n",
      "3477/3477 [==============================] - 0s 90us/step - loss: 0.4197 - val_loss: 0.5869\n",
      "Epoch 897/1200\n",
      "3477/3477 [==============================] - 0s 120us/step - loss: 0.4214 - val_loss: 0.6098\n",
      "Epoch 898/1200\n",
      "3477/3477 [==============================] - 0s 95us/step - loss: 0.4217 - val_loss: 0.5669\n",
      "Epoch 899/1200\n",
      "3477/3477 [==============================] - 0s 88us/step - loss: 0.4215 - val_loss: 0.5910\n",
      "Epoch 900/1200\n",
      "3477/3477 [==============================] - 0s 90us/step - loss: 0.4229 - val_loss: 0.5594\n",
      "Epoch 901/1200\n",
      "3477/3477 [==============================] - 0s 99us/step - loss: 0.4055 - val_loss: 0.5323\n",
      "Epoch 902/1200\n",
      "3477/3477 [==============================] - 0s 83us/step - loss: 0.4112 - val_loss: 0.5486\n",
      "Epoch 903/1200\n",
      "3477/3477 [==============================] - 0s 82us/step - loss: 0.4091 - val_loss: 0.5627\n",
      "Epoch 904/1200\n",
      "3477/3477 [==============================] - 0s 78us/step - loss: 0.4081 - val_loss: 0.5645\n",
      "Epoch 905/1200\n",
      "3477/3477 [==============================] - 0s 81us/step - loss: 0.4143 - val_loss: 0.5622\n",
      "Epoch 906/1200\n",
      "3477/3477 [==============================] - 0s 79us/step - loss: 0.4150 - val_loss: 0.5562\n",
      "Epoch 907/1200\n",
      "3477/3477 [==============================] - 0s 87us/step - loss: 0.4145 - val_loss: 0.5567\n",
      "Epoch 908/1200\n",
      "3477/3477 [==============================] - 0s 80us/step - loss: 0.4168 - val_loss: 0.5493\n",
      "Epoch 909/1200\n",
      "3477/3477 [==============================] - 0s 85us/step - loss: 0.4103 - val_loss: 0.5610\n",
      "Epoch 910/1200\n",
      "3477/3477 [==============================] - 0s 79us/step - loss: 0.4108 - val_loss: 0.5553\n",
      "Epoch 911/1200\n",
      "3477/3477 [==============================] - 0s 77us/step - loss: 0.4135 - val_loss: 0.5652\n",
      "Epoch 912/1200\n",
      "3477/3477 [==============================] - 0s 79us/step - loss: 0.4206 - val_loss: 0.5433\n",
      "Epoch 913/1200\n",
      "3477/3477 [==============================] - 0s 82us/step - loss: 0.4083 - val_loss: 0.5701\n",
      "Epoch 914/1200\n",
      "3477/3477 [==============================] - 0s 79us/step - loss: 0.4064 - val_loss: 0.5973\n",
      "Epoch 915/1200\n",
      "3477/3477 [==============================] - 0s 78us/step - loss: 0.4168 - val_loss: 0.5812\n",
      "Epoch 916/1200\n",
      "3477/3477 [==============================] - 0s 87us/step - loss: 0.4355 - val_loss: 0.5803\n",
      "Epoch 917/1200\n",
      "3477/3477 [==============================] - 0s 80us/step - loss: 0.4120 - val_loss: 0.5560\n",
      "Epoch 918/1200\n",
      "3477/3477 [==============================] - 0s 81us/step - loss: 0.4223 - val_loss: 0.5717\n",
      "Epoch 919/1200\n",
      "3477/3477 [==============================] - 0s 78us/step - loss: 0.4115 - val_loss: 0.5541\n",
      "Epoch 920/1200\n",
      "3477/3477 [==============================] - 0s 78us/step - loss: 0.4224 - val_loss: 0.6069\n",
      "Epoch 921/1200\n",
      "3477/3477 [==============================] - 0s 78us/step - loss: 0.4102 - val_loss: 0.5558\n",
      "Epoch 922/1200\n",
      "3477/3477 [==============================] - 0s 80us/step - loss: 0.4128 - val_loss: 0.5428\n",
      "Epoch 923/1200\n",
      "3477/3477 [==============================] - 0s 80us/step - loss: 0.4201 - val_loss: 0.5721\n",
      "Epoch 924/1200\n",
      "3477/3477 [==============================] - 0s 79us/step - loss: 0.4015 - val_loss: 0.5620\n",
      "Epoch 925/1200\n",
      "3477/3477 [==============================] - 0s 79us/step - loss: 0.4044 - val_loss: 0.5760\n",
      "Epoch 926/1200\n",
      "3477/3477 [==============================] - 0s 77us/step - loss: 0.4041 - val_loss: 0.5933\n",
      "Epoch 927/1200\n",
      "3477/3477 [==============================] - 0s 90us/step - loss: 0.4171 - val_loss: 0.5818\n",
      "Epoch 928/1200\n",
      "3477/3477 [==============================] - 0s 78us/step - loss: 0.3994 - val_loss: 0.5499\n",
      "Epoch 929/1200\n",
      "3477/3477 [==============================] - 0s 80us/step - loss: 0.4053 - val_loss: 0.5632\n",
      "Epoch 930/1200\n",
      "3477/3477 [==============================] - 0s 79us/step - loss: 0.4107 - val_loss: 0.5404\n",
      "Epoch 931/1200\n",
      "3477/3477 [==============================] - 0s 77us/step - loss: 0.4112 - val_loss: 0.6315\n",
      "Epoch 932/1200\n",
      "3477/3477 [==============================] - 0s 81us/step - loss: 0.4175 - val_loss: 0.5416\n",
      "Epoch 933/1200\n",
      "3477/3477 [==============================] - 0s 83us/step - loss: 0.4015 - val_loss: 0.5760\n",
      "Epoch 934/1200\n",
      "3477/3477 [==============================] - 0s 79us/step - loss: 0.4065 - val_loss: 0.5507\n",
      "Epoch 935/1200\n",
      "3477/3477 [==============================] - 0s 78us/step - loss: 0.4155 - val_loss: 0.5654\n",
      "Epoch 936/1200\n",
      "3477/3477 [==============================] - 0s 78us/step - loss: 0.4112 - val_loss: 0.5458\n",
      "Epoch 937/1200\n",
      "3477/3477 [==============================] - 0s 80us/step - loss: 0.4052 - val_loss: 0.5379\n",
      "Epoch 938/1200\n",
      "3477/3477 [==============================] - 0s 79us/step - loss: 0.4067 - val_loss: 0.5580\n",
      "Epoch 939/1200\n",
      "3477/3477 [==============================] - 0s 79us/step - loss: 0.4082 - val_loss: 0.5462\n",
      "Epoch 940/1200\n",
      "3477/3477 [==============================] - 0s 79us/step - loss: 0.3988 - val_loss: 0.5497\n",
      "Epoch 941/1200\n",
      "3477/3477 [==============================] - 0s 79us/step - loss: 0.4008 - val_loss: 0.5500\n",
      "Epoch 942/1200\n",
      "3477/3477 [==============================] - 0s 79us/step - loss: 0.3967 - val_loss: 0.5601\n",
      "Epoch 943/1200\n",
      "3477/3477 [==============================] - 0s 78us/step - loss: 0.3994 - val_loss: 0.5461\n",
      "Epoch 944/1200\n",
      "3477/3477 [==============================] - 0s 79us/step - loss: 0.3990 - val_loss: 0.5511\n",
      "Epoch 945/1200\n",
      "3477/3477 [==============================] - 0s 77us/step - loss: 0.4278 - val_loss: 0.5723\n",
      "Epoch 946/1200\n",
      "3477/3477 [==============================] - 0s 80us/step - loss: 0.4000 - val_loss: 0.5358\n",
      "Epoch 947/1200\n",
      "3477/3477 [==============================] - 0s 79us/step - loss: 0.4009 - val_loss: 0.5495\n",
      "Epoch 948/1200\n",
      "3477/3477 [==============================] - 0s 79us/step - loss: 0.4069 - val_loss: 0.5524\n",
      "Epoch 949/1200\n",
      "3477/3477 [==============================] - 0s 77us/step - loss: 0.4004 - val_loss: 0.5627\n",
      "Epoch 950/1200\n",
      "3477/3477 [==============================] - 0s 78us/step - loss: 0.3924 - val_loss: 0.5652\n",
      "Epoch 951/1200\n",
      "3477/3477 [==============================] - 0s 85us/step - loss: 0.4121 - val_loss: 0.5819\n",
      "Epoch 952/1200\n",
      "3477/3477 [==============================] - 0s 86us/step - loss: 0.3998 - val_loss: 0.5491\n",
      "Epoch 953/1200\n",
      "3477/3477 [==============================] - 0s 95us/step - loss: 0.3984 - val_loss: 0.6084\n",
      "Epoch 954/1200\n",
      "3477/3477 [==============================] - 0s 97us/step - loss: 0.4003 - val_loss: 0.5673\n",
      "Epoch 955/1200\n",
      "3477/3477 [==============================] - 0s 94us/step - loss: 0.4050 - val_loss: 0.5416\n",
      "Epoch 956/1200\n",
      "3477/3477 [==============================] - 0s 88us/step - loss: 0.4036 - val_loss: 0.5565\n",
      "Epoch 957/1200\n",
      "3477/3477 [==============================] - 0s 105us/step - loss: 0.3864 - val_loss: 0.5458\n",
      "Epoch 958/1200\n",
      "3477/3477 [==============================] - 0s 93us/step - loss: 0.3969 - val_loss: 0.5481\n",
      "Epoch 959/1200\n",
      "3477/3477 [==============================] - 0s 78us/step - loss: 0.4048 - val_loss: 0.5661\n",
      "Epoch 960/1200\n",
      "3477/3477 [==============================] - 0s 81us/step - loss: 0.4057 - val_loss: 0.5359\n",
      "Epoch 961/1200\n",
      "3477/3477 [==============================] - 0s 81us/step - loss: 0.4224 - val_loss: 0.5640\n",
      "Epoch 962/1200\n",
      "3477/3477 [==============================] - 0s 79us/step - loss: 0.4015 - val_loss: 0.5362\n",
      "Epoch 963/1200\n",
      "3477/3477 [==============================] - 0s 80us/step - loss: 0.3993 - val_loss: 0.5786\n",
      "Epoch 964/1200\n",
      "3477/3477 [==============================] - 0s 80us/step - loss: 0.3925 - val_loss: 0.5741\n",
      "Epoch 965/1200\n",
      "3477/3477 [==============================] - 0s 82us/step - loss: 0.4107 - val_loss: 0.5533\n",
      "Epoch 966/1200\n",
      "3477/3477 [==============================] - 0s 79us/step - loss: 0.4029 - val_loss: 0.5423\n",
      "Epoch 967/1200\n",
      "3477/3477 [==============================] - 0s 78us/step - loss: 0.3919 - val_loss: 0.5458\n",
      "Epoch 968/1200\n",
      "3477/3477 [==============================] - 0s 80us/step - loss: 0.4014 - val_loss: 0.5482\n",
      "Epoch 969/1200\n",
      "3477/3477 [==============================] - 0s 80us/step - loss: 0.4027 - val_loss: 0.5567\n",
      "Epoch 970/1200\n",
      "3477/3477 [==============================] - 0s 79us/step - loss: 0.4110 - val_loss: 0.5429\n",
      "Epoch 971/1200\n",
      "3477/3477 [==============================] - 0s 81us/step - loss: 0.4074 - val_loss: 0.5449\n",
      "Epoch 972/1200\n",
      "3477/3477 [==============================] - 0s 80us/step - loss: 0.4025 - val_loss: 0.5609\n",
      "Epoch 973/1200\n",
      "3477/3477 [==============================] - 0s 80us/step - loss: 0.4036 - val_loss: 0.5476\n",
      "Epoch 974/1200\n",
      "3477/3477 [==============================] - 0s 79us/step - loss: 0.4014 - val_loss: 0.5504\n",
      "Epoch 975/1200\n",
      "3477/3477 [==============================] - 0s 78us/step - loss: 0.3957 - val_loss: 0.5691\n",
      "Epoch 976/1200\n",
      "3477/3477 [==============================] - 0s 78us/step - loss: 0.3984 - val_loss: 0.5379\n",
      "Epoch 977/1200\n",
      "3477/3477 [==============================] - 0s 77us/step - loss: 0.4027 - val_loss: 0.5439\n",
      "Epoch 978/1200\n",
      "3477/3477 [==============================] - 0s 77us/step - loss: 0.3921 - val_loss: 0.5385\n",
      "Epoch 979/1200\n",
      "3477/3477 [==============================] - 0s 79us/step - loss: 0.3928 - val_loss: 0.5510\n",
      "Epoch 980/1200\n",
      "3477/3477 [==============================] - 0s 80us/step - loss: 0.3955 - val_loss: 0.5369\n",
      "Epoch 981/1200\n",
      "3477/3477 [==============================] - 0s 78us/step - loss: 0.3931 - val_loss: 0.5508\n",
      "Epoch 982/1200\n",
      "3477/3477 [==============================] - 0s 77us/step - loss: 0.3919 - val_loss: 0.5393\n",
      "Epoch 983/1200\n",
      "3477/3477 [==============================] - 0s 78us/step - loss: 0.3923 - val_loss: 0.6375\n",
      "Epoch 984/1200\n",
      "3477/3477 [==============================] - 0s 81us/step - loss: 0.4046 - val_loss: 0.5660\n",
      "Epoch 985/1200\n",
      "3477/3477 [==============================] - 0s 84us/step - loss: 0.4054 - val_loss: 0.5683\n",
      "Epoch 986/1200\n",
      "3477/3477 [==============================] - 0s 77us/step - loss: 0.3890 - val_loss: 0.5282\n",
      "Epoch 987/1200\n",
      "3477/3477 [==============================] - 0s 78us/step - loss: 0.3890 - val_loss: 0.5383\n",
      "Epoch 988/1200\n",
      "3477/3477 [==============================] - 0s 79us/step - loss: 0.4020 - val_loss: 0.5474\n",
      "Epoch 989/1200\n",
      "3477/3477 [==============================] - 0s 81us/step - loss: 0.4065 - val_loss: 0.5391\n",
      "Epoch 990/1200\n",
      "3477/3477 [==============================] - 0s 78us/step - loss: 0.4097 - val_loss: 0.5403\n",
      "Epoch 991/1200\n",
      "3477/3477 [==============================] - 0s 77us/step - loss: 0.3934 - val_loss: 0.5439\n",
      "Epoch 992/1200\n",
      "3477/3477 [==============================] - 0s 79us/step - loss: 0.3895 - val_loss: 0.5387\n",
      "Epoch 993/1200\n",
      "3477/3477 [==============================] - 0s 82us/step - loss: 0.3937 - val_loss: 0.5523\n",
      "Epoch 994/1200\n",
      "3477/3477 [==============================] - 0s 84us/step - loss: 0.3989 - val_loss: 0.5609\n",
      "Epoch 995/1200\n",
      "3477/3477 [==============================] - 0s 78us/step - loss: 0.3989 - val_loss: 0.5737\n",
      "Epoch 996/1200\n",
      "3477/3477 [==============================] - 0s 81us/step - loss: 0.3958 - val_loss: 0.5454\n",
      "Epoch 997/1200\n",
      "3477/3477 [==============================] - 0s 79us/step - loss: 0.3978 - val_loss: 0.5517\n",
      "Epoch 998/1200\n",
      "3477/3477 [==============================] - 0s 79us/step - loss: 0.3893 - val_loss: 0.5561\n",
      "Epoch 999/1200\n",
      "3477/3477 [==============================] - 0s 80us/step - loss: 0.3885 - val_loss: 0.5319\n",
      "Epoch 1000/1200\n",
      "3477/3477 [==============================] - 0s 79us/step - loss: 0.4004 - val_loss: 0.5327\n",
      "Epoch 1001/1200\n",
      "3477/3477 [==============================] - 0s 84us/step - loss: 0.3877 - val_loss: 0.5202\n",
      "Epoch 1002/1200\n",
      "3477/3477 [==============================] - 0s 97us/step - loss: 0.3915 - val_loss: 0.5137\n",
      "Epoch 1003/1200\n",
      "3477/3477 [==============================] - 0s 78us/step - loss: 0.3935 - val_loss: 0.5272\n",
      "Epoch 1004/1200\n",
      "3477/3477 [==============================] - 0s 79us/step - loss: 0.3909 - val_loss: 0.5450\n",
      "Epoch 1005/1200\n",
      "3477/3477 [==============================] - 0s 79us/step - loss: 0.3976 - val_loss: 0.5501\n",
      "Epoch 1006/1200\n",
      "3477/3477 [==============================] - 0s 81us/step - loss: 0.3934 - val_loss: 0.5453\n",
      "Epoch 1007/1200\n",
      "3477/3477 [==============================] - 0s 81us/step - loss: 0.3947 - val_loss: 0.5426\n",
      "Epoch 1008/1200\n",
      "3477/3477 [==============================] - 0s 83us/step - loss: 0.4090 - val_loss: 0.6010\n",
      "Epoch 1009/1200\n",
      "3477/3477 [==============================] - 0s 99us/step - loss: 0.3939 - val_loss: 0.5432\n",
      "Epoch 1010/1200\n",
      "3477/3477 [==============================] - 0s 98us/step - loss: 0.3947 - val_loss: 0.5364\n",
      "Epoch 1011/1200\n",
      "3477/3477 [==============================] - 0s 97us/step - loss: 0.3985 - val_loss: 0.5590\n",
      "Epoch 1012/1200\n",
      "3477/3477 [==============================] - 0s 82us/step - loss: 0.4152 - val_loss: 0.5498\n",
      "Epoch 1013/1200\n",
      "3477/3477 [==============================] - 0s 108us/step - loss: 0.4145 - val_loss: 0.5407\n",
      "Epoch 1014/1200\n",
      "3477/3477 [==============================] - 0s 94us/step - loss: 0.3869 - val_loss: 0.5561\n",
      "Epoch 1015/1200\n",
      "3477/3477 [==============================] - 0s 86us/step - loss: 0.3873 - val_loss: 0.5326\n",
      "Epoch 1016/1200\n",
      "3477/3477 [==============================] - 0s 80us/step - loss: 0.3875 - val_loss: 0.5689\n",
      "Epoch 1017/1200\n",
      "3477/3477 [==============================] - 0s 81us/step - loss: 0.3869 - val_loss: 0.5406\n",
      "Epoch 1018/1200\n",
      "3477/3477 [==============================] - 0s 79us/step - loss: 0.3918 - val_loss: 0.5397\n",
      "Epoch 1019/1200\n",
      "3477/3477 [==============================] - 0s 79us/step - loss: 0.3908 - val_loss: 0.5625\n",
      "Epoch 1020/1200\n",
      "3477/3477 [==============================] - 0s 82us/step - loss: 0.3966 - val_loss: 0.5420\n",
      "Epoch 1021/1200\n",
      "3477/3477 [==============================] - 0s 80us/step - loss: 0.3831 - val_loss: 0.5357\n",
      "Epoch 1022/1200\n",
      "3477/3477 [==============================] - 0s 79us/step - loss: 0.3885 - val_loss: 0.5311\n",
      "Epoch 1023/1200\n",
      "3477/3477 [==============================] - 0s 80us/step - loss: 0.3809 - val_loss: 0.5303\n",
      "Epoch 1024/1200\n",
      "3477/3477 [==============================] - 0s 80us/step - loss: 0.3905 - val_loss: 0.5520\n",
      "Epoch 1025/1200\n",
      "3477/3477 [==============================] - 0s 81us/step - loss: 0.3902 - val_loss: 0.5328\n",
      "Epoch 1026/1200\n",
      "3477/3477 [==============================] - 0s 79us/step - loss: 0.3881 - val_loss: 0.5461\n",
      "Epoch 1027/1200\n",
      "3477/3477 [==============================] - 0s 78us/step - loss: 0.3837 - val_loss: 0.5698\n",
      "Epoch 1028/1200\n",
      "3477/3477 [==============================] - 0s 77us/step - loss: 0.3833 - val_loss: 0.5403\n",
      "Epoch 1029/1200\n",
      "3477/3477 [==============================] - 0s 79us/step - loss: 0.3898 - val_loss: 0.5511\n",
      "Epoch 1030/1200\n",
      "3477/3477 [==============================] - 0s 81us/step - loss: 0.3944 - val_loss: 0.5907\n",
      "Epoch 1031/1200\n",
      "3477/3477 [==============================] - 0s 78us/step - loss: 0.3941 - val_loss: 0.5399\n",
      "Epoch 1032/1200\n",
      "3477/3477 [==============================] - 0s 79us/step - loss: 0.3965 - val_loss: 0.5341\n",
      "Epoch 1033/1200\n",
      "3477/3477 [==============================] - 0s 77us/step - loss: 0.3864 - val_loss: 0.5415\n",
      "Epoch 1034/1200\n",
      "3477/3477 [==============================] - 0s 78us/step - loss: 0.3914 - val_loss: 0.5578\n",
      "Epoch 1035/1200\n",
      "3477/3477 [==============================] - 0s 79us/step - loss: 0.3885 - val_loss: 0.5411\n",
      "Epoch 1036/1200\n",
      "3477/3477 [==============================] - 0s 82us/step - loss: 0.3859 - val_loss: 0.5447\n",
      "Epoch 1037/1200\n",
      "3477/3477 [==============================] - 0s 77us/step - loss: 0.3952 - val_loss: 0.5538\n",
      "Epoch 1038/1200\n",
      "3477/3477 [==============================] - 0s 77us/step - loss: 0.3785 - val_loss: 0.5991\n",
      "Epoch 1039/1200\n",
      "3477/3477 [==============================] - 0s 79us/step - loss: 0.3909 - val_loss: 0.5472\n",
      "Epoch 1040/1200\n",
      "3477/3477 [==============================] - 0s 81us/step - loss: 0.3901 - val_loss: 0.5285\n",
      "Epoch 1041/1200\n",
      "3477/3477 [==============================] - 0s 79us/step - loss: 0.3894 - val_loss: 0.5562\n",
      "Epoch 1042/1200\n",
      "3477/3477 [==============================] - 0s 76us/step - loss: 0.4022 - val_loss: 0.5259\n",
      "Epoch 1043/1200\n",
      "3477/3477 [==============================] - 0s 78us/step - loss: 0.3827 - val_loss: 0.5412\n",
      "Epoch 1044/1200\n",
      "3477/3477 [==============================] - 0s 79us/step - loss: 0.4171 - val_loss: 0.5450\n",
      "Epoch 1045/1200\n",
      "3477/3477 [==============================] - 0s 79us/step - loss: 0.3792 - val_loss: 0.5465\n",
      "Epoch 1046/1200\n",
      "3477/3477 [==============================] - 0s 81us/step - loss: 0.3902 - val_loss: 0.5450\n",
      "Epoch 1047/1200\n",
      "3477/3477 [==============================] - 0s 97us/step - loss: 0.3940 - val_loss: 0.5380\n",
      "Epoch 1048/1200\n",
      "3477/3477 [==============================] - 0s 77us/step - loss: 0.3898 - val_loss: 0.5278\n",
      "Epoch 1049/1200\n",
      "3477/3477 [==============================] - 0s 81us/step - loss: 0.3864 - val_loss: 0.5237\n",
      "Epoch 1050/1200\n",
      "3477/3477 [==============================] - 0s 77us/step - loss: 0.3850 - val_loss: 0.5497\n",
      "Epoch 1051/1200\n",
      "3477/3477 [==============================] - 0s 99us/step - loss: 0.3823 - val_loss: 0.5306\n",
      "Epoch 1052/1200\n",
      "3477/3477 [==============================] - 0s 78us/step - loss: 0.4008 - val_loss: 0.5397\n",
      "Epoch 1053/1200\n",
      "3477/3477 [==============================] - 0s 78us/step - loss: 0.3886 - val_loss: 0.5456\n",
      "Epoch 1054/1200\n",
      "3477/3477 [==============================] - 0s 81us/step - loss: 0.3865 - val_loss: 0.5336\n",
      "Epoch 1055/1200\n",
      "3477/3477 [==============================] - 0s 79us/step - loss: 0.3824 - val_loss: 0.5349\n",
      "Epoch 1056/1200\n",
      "3477/3477 [==============================] - 0s 79us/step - loss: 0.3894 - val_loss: 0.5316\n",
      "Epoch 1057/1200\n",
      "3477/3477 [==============================] - 0s 84us/step - loss: 0.3849 - val_loss: 0.5332\n",
      "Epoch 1058/1200\n",
      "3477/3477 [==============================] - 0s 87us/step - loss: 0.3791 - val_loss: 0.5265\n",
      "Epoch 1059/1200\n",
      "3477/3477 [==============================] - 0s 91us/step - loss: 0.3857 - val_loss: 0.5444\n",
      "Epoch 1060/1200\n",
      "3477/3477 [==============================] - 0s 77us/step - loss: 0.3856 - val_loss: 0.5525\n",
      "Epoch 1061/1200\n",
      "3477/3477 [==============================] - 0s 78us/step - loss: 0.3874 - val_loss: 0.5398\n",
      "Epoch 1062/1200\n",
      "3477/3477 [==============================] - 0s 80us/step - loss: 0.4012 - val_loss: 0.5290\n",
      "Epoch 1063/1200\n",
      "3477/3477 [==============================] - 0s 82us/step - loss: 0.3875 - val_loss: 0.5316\n",
      "Epoch 1064/1200\n",
      "3477/3477 [==============================] - 0s 85us/step - loss: 0.3847 - val_loss: 0.5448\n",
      "Epoch 1065/1200\n",
      "3477/3477 [==============================] - 0s 100us/step - loss: 0.3900 - val_loss: 0.5187\n",
      "Epoch 1066/1200\n",
      "3477/3477 [==============================] - 0s 100us/step - loss: 0.3848 - val_loss: 0.5253\n",
      "Epoch 1067/1200\n",
      "3477/3477 [==============================] - 0s 103us/step - loss: 0.3885 - val_loss: 0.5581\n",
      "Epoch 1068/1200\n",
      "3477/3477 [==============================] - 0s 92us/step - loss: 0.3928 - val_loss: 0.5273\n",
      "Epoch 1069/1200\n",
      "3477/3477 [==============================] - 0s 99us/step - loss: 0.3854 - val_loss: 0.5236\n",
      "Epoch 1070/1200\n",
      "3477/3477 [==============================] - 0s 85us/step - loss: 0.4190 - val_loss: 0.5441\n",
      "Epoch 1071/1200\n",
      "3477/3477 [==============================] - 0s 99us/step - loss: 0.3830 - val_loss: 0.5242\n",
      "Epoch 1072/1200\n",
      "3477/3477 [==============================] - 0s 77us/step - loss: 0.3952 - val_loss: 0.5684\n",
      "Epoch 1073/1200\n",
      "3477/3477 [==============================] - 0s 81us/step - loss: 0.3815 - val_loss: 0.5629\n",
      "Epoch 1074/1200\n",
      "3477/3477 [==============================] - 0s 77us/step - loss: 0.3890 - val_loss: 0.5404\n",
      "Epoch 1075/1200\n",
      "3477/3477 [==============================] - 0s 77us/step - loss: 0.3882 - val_loss: 0.5605\n",
      "Epoch 1076/1200\n",
      "3477/3477 [==============================] - 0s 82us/step - loss: 0.3828 - val_loss: 0.5486\n",
      "Epoch 1077/1200\n",
      "3477/3477 [==============================] - 0s 81us/step - loss: 0.3834 - val_loss: 0.5427\n",
      "Epoch 1078/1200\n",
      "3477/3477 [==============================] - 0s 81us/step - loss: 0.3866 - val_loss: 0.5241\n",
      "Epoch 1079/1200\n",
      "3477/3477 [==============================] - 0s 80us/step - loss: 0.3811 - val_loss: 0.5316\n",
      "Epoch 1080/1200\n",
      "3477/3477 [==============================] - 0s 79us/step - loss: 0.3842 - val_loss: 0.5993\n",
      "Epoch 1081/1200\n",
      "3477/3477 [==============================] - 0s 78us/step - loss: 0.3797 - val_loss: 0.5748\n",
      "Epoch 1082/1200\n",
      "3477/3477 [==============================] - 0s 99us/step - loss: 0.3913 - val_loss: 0.5349\n",
      "Epoch 1083/1200\n",
      "3477/3477 [==============================] - 0s 77us/step - loss: 0.3839 - val_loss: 0.5573\n",
      "Epoch 1084/1200\n",
      "3477/3477 [==============================] - 0s 86us/step - loss: 0.3881 - val_loss: 0.5421\n",
      "Epoch 1085/1200\n",
      "3477/3477 [==============================] - 0s 80us/step - loss: 0.3784 - val_loss: 0.5588\n",
      "Epoch 1086/1200\n",
      "3477/3477 [==============================] - 0s 79us/step - loss: 0.3791 - val_loss: 0.5388\n",
      "Epoch 1087/1200\n",
      "3477/3477 [==============================] - 0s 79us/step - loss: 0.3818 - val_loss: 0.5182\n",
      "Epoch 1088/1200\n",
      "3477/3477 [==============================] - 0s 82us/step - loss: 0.3876 - val_loss: 0.5302\n",
      "Epoch 1089/1200\n",
      "3477/3477 [==============================] - 0s 77us/step - loss: 0.3870 - val_loss: 0.5223\n",
      "Epoch 1090/1200\n",
      "3477/3477 [==============================] - 0s 79us/step - loss: 0.3865 - val_loss: 0.5430\n",
      "Epoch 1091/1200\n",
      "3477/3477 [==============================] - 0s 96us/step - loss: 0.3949 - val_loss: 0.5796\n",
      "Epoch 1092/1200\n",
      "3477/3477 [==============================] - 0s 77us/step - loss: 0.3779 - val_loss: 0.5262\n",
      "Epoch 1093/1200\n",
      "3477/3477 [==============================] - 0s 77us/step - loss: 0.3830 - val_loss: 0.5196\n",
      "Epoch 1094/1200\n",
      "3477/3477 [==============================] - 0s 78us/step - loss: 0.3869 - val_loss: 0.5362\n",
      "Epoch 1095/1200\n",
      "3477/3477 [==============================] - 0s 79us/step - loss: 0.3872 - val_loss: 0.5133\n",
      "Epoch 1096/1200\n",
      "3477/3477 [==============================] - 0s 77us/step - loss: 0.3794 - val_loss: 0.5145\n",
      "Epoch 1097/1200\n",
      "3477/3477 [==============================] - 0s 79us/step - loss: 0.3902 - val_loss: 0.5858\n",
      "Epoch 1098/1200\n",
      "3477/3477 [==============================] - 0s 103us/step - loss: 0.3881 - val_loss: 0.5072\n",
      "Epoch 1099/1200\n",
      "3477/3477 [==============================] - 0s 82us/step - loss: 0.3753 - val_loss: 0.5287\n",
      "Epoch 1100/1200\n",
      "3477/3477 [==============================] - 0s 81us/step - loss: 0.3814 - val_loss: 0.5380\n",
      "Epoch 1101/1200\n",
      "3477/3477 [==============================] - 0s 80us/step - loss: 0.3813 - val_loss: 0.5286\n",
      "Epoch 1102/1200\n",
      "3477/3477 [==============================] - 0s 80us/step - loss: 0.3836 - val_loss: 0.5646\n",
      "Epoch 1103/1200\n",
      "3477/3477 [==============================] - 0s 81us/step - loss: 0.3965 - val_loss: 0.5221\n",
      "Epoch 1104/1200\n",
      "3477/3477 [==============================] - 0s 81us/step - loss: 0.3752 - val_loss: 0.5382\n",
      "Epoch 1105/1200\n",
      "3477/3477 [==============================] - 0s 79us/step - loss: 0.3824 - val_loss: 0.5246\n",
      "Epoch 1106/1200\n",
      "3477/3477 [==============================] - 0s 83us/step - loss: 0.3919 - val_loss: 0.5431\n",
      "Epoch 1107/1200\n",
      "3477/3477 [==============================] - 0s 76us/step - loss: 0.3894 - val_loss: 0.5271\n",
      "Epoch 1108/1200\n",
      "3477/3477 [==============================] - 0s 79us/step - loss: 0.3814 - val_loss: 0.6490\n",
      "Epoch 1109/1200\n",
      "3477/3477 [==============================] - 0s 83us/step - loss: 0.3931 - val_loss: 0.5598\n",
      "Epoch 1110/1200\n",
      "3477/3477 [==============================] - 0s 79us/step - loss: 0.3975 - val_loss: 0.5199\n",
      "Epoch 1111/1200\n",
      "3477/3477 [==============================] - 0s 78us/step - loss: 0.3749 - val_loss: 0.5261\n",
      "Epoch 1112/1200\n",
      "3477/3477 [==============================] - 0s 79us/step - loss: 0.3841 - val_loss: 0.5434\n",
      "Epoch 1113/1200\n",
      "3477/3477 [==============================] - 0s 85us/step - loss: 0.3834 - val_loss: 0.5293\n",
      "Epoch 1114/1200\n",
      "3477/3477 [==============================] - 0s 80us/step - loss: 0.3852 - val_loss: 0.5416\n",
      "Epoch 1115/1200\n",
      "3477/3477 [==============================] - 0s 81us/step - loss: 0.3832 - val_loss: 0.5304\n",
      "Epoch 1116/1200\n",
      "3477/3477 [==============================] - 0s 79us/step - loss: 0.3902 - val_loss: 0.5336\n",
      "Epoch 1117/1200\n",
      "3477/3477 [==============================] - 0s 79us/step - loss: 0.3845 - val_loss: 0.5767\n",
      "Epoch 1118/1200\n",
      "3477/3477 [==============================] - 0s 81us/step - loss: 0.3805 - val_loss: 0.5367\n",
      "Epoch 1119/1200\n",
      "3477/3477 [==============================] - 0s 79us/step - loss: 0.3868 - val_loss: 0.5353\n",
      "Epoch 1120/1200\n",
      "3477/3477 [==============================] - 0s 98us/step - loss: 0.3800 - val_loss: 0.5421\n",
      "Epoch 1121/1200\n",
      "3477/3477 [==============================] - 0s 118us/step - loss: 0.3952 - val_loss: 0.5220\n",
      "Epoch 1122/1200\n",
      "3477/3477 [==============================] - 0s 94us/step - loss: 0.3760 - val_loss: 0.5295\n",
      "Epoch 1123/1200\n",
      "3477/3477 [==============================] - 0s 81us/step - loss: 0.3751 - val_loss: 0.5166\n",
      "Epoch 1124/1200\n",
      "3477/3477 [==============================] - 0s 101us/step - loss: 0.3878 - val_loss: 0.5598\n",
      "Epoch 1125/1200\n",
      "3477/3477 [==============================] - 0s 88us/step - loss: 0.3862 - val_loss: 0.5182\n",
      "Epoch 1126/1200\n",
      "3477/3477 [==============================] - 0s 79us/step - loss: 0.3905 - val_loss: 0.5192\n",
      "Epoch 1127/1200\n",
      "3477/3477 [==============================] - 0s 78us/step - loss: 0.3816 - val_loss: 0.5299\n",
      "Epoch 1128/1200\n",
      "3477/3477 [==============================] - 0s 79us/step - loss: 0.3780 - val_loss: 0.5393\n",
      "Epoch 1129/1200\n",
      "3477/3477 [==============================] - 0s 78us/step - loss: 0.3876 - val_loss: 0.5318\n",
      "Epoch 1130/1200\n",
      "3477/3477 [==============================] - 0s 81us/step - loss: 0.3788 - val_loss: 0.5660\n",
      "Epoch 1131/1200\n",
      "3477/3477 [==============================] - 0s 80us/step - loss: 0.3863 - val_loss: 0.5292\n",
      "Epoch 1132/1200\n",
      "3477/3477 [==============================] - 0s 79us/step - loss: 0.3810 - val_loss: 0.5272\n",
      "Epoch 1133/1200\n",
      "3477/3477 [==============================] - 0s 79us/step - loss: 0.3813 - val_loss: 0.5311\n",
      "Epoch 1134/1200\n",
      "3477/3477 [==============================] - 0s 79us/step - loss: 0.3837 - val_loss: 0.5329\n",
      "Epoch 1135/1200\n",
      "3477/3477 [==============================] - 0s 79us/step - loss: 0.3812 - val_loss: 0.5423\n",
      "Epoch 1136/1200\n",
      "3477/3477 [==============================] - 0s 80us/step - loss: 0.3778 - val_loss: 0.5179\n",
      "Epoch 1137/1200\n",
      "3477/3477 [==============================] - 0s 78us/step - loss: 0.3805 - val_loss: 0.5193\n",
      "Epoch 1138/1200\n",
      "3477/3477 [==============================] - 0s 77us/step - loss: 0.3806 - val_loss: 0.5450\n",
      "Epoch 1139/1200\n",
      "3477/3477 [==============================] - 0s 78us/step - loss: 0.3798 - val_loss: 0.5392\n",
      "Epoch 1140/1200\n",
      "3477/3477 [==============================] - 0s 75us/step - loss: 0.3751 - val_loss: 0.5400\n",
      "Epoch 1141/1200\n",
      "3477/3477 [==============================] - 0s 85us/step - loss: 0.3757 - val_loss: 0.5155\n",
      "Epoch 1142/1200\n",
      "3477/3477 [==============================] - 0s 82us/step - loss: 0.3761 - val_loss: 0.5614\n",
      "Epoch 1143/1200\n",
      "3477/3477 [==============================] - 0s 79us/step - loss: 0.3748 - val_loss: 0.5634\n",
      "Epoch 1144/1200\n",
      "3477/3477 [==============================] - 0s 78us/step - loss: 0.3759 - val_loss: 0.5427\n",
      "Epoch 1145/1200\n",
      "3477/3477 [==============================] - 0s 77us/step - loss: 0.3795 - val_loss: 0.5920\n",
      "Epoch 1146/1200\n",
      "3477/3477 [==============================] - 0s 85us/step - loss: 0.3854 - val_loss: 0.5389\n",
      "Epoch 1147/1200\n",
      "3477/3477 [==============================] - 0s 77us/step - loss: 0.3844 - val_loss: 0.5266\n",
      "Epoch 1148/1200\n",
      "3477/3477 [==============================] - 0s 80us/step - loss: 0.3771 - val_loss: 0.5328\n",
      "Epoch 1149/1200\n",
      "3477/3477 [==============================] - 0s 78us/step - loss: 0.3749 - val_loss: 0.5384\n",
      "Epoch 1150/1200\n",
      "3477/3477 [==============================] - 0s 81us/step - loss: 0.3878 - val_loss: 0.5328\n",
      "Epoch 1151/1200\n",
      "3477/3477 [==============================] - 0s 80us/step - loss: 0.3807 - val_loss: 0.5297\n",
      "Epoch 1152/1200\n",
      "3477/3477 [==============================] - 0s 81us/step - loss: 0.3885 - val_loss: 0.5219\n",
      "Epoch 1153/1200\n",
      "3477/3477 [==============================] - 0s 86us/step - loss: 0.3832 - val_loss: 0.5220\n",
      "Epoch 1154/1200\n",
      "3477/3477 [==============================] - 0s 76us/step - loss: 0.3867 - val_loss: 0.5334\n",
      "Epoch 1155/1200\n",
      "3477/3477 [==============================] - 0s 92us/step - loss: 0.3832 - val_loss: 0.5408\n",
      "Epoch 1156/1200\n",
      "3477/3477 [==============================] - 0s 76us/step - loss: 0.3716 - val_loss: 0.5422\n",
      "Epoch 1157/1200\n",
      "3477/3477 [==============================] - 0s 79us/step - loss: 0.3902 - val_loss: 0.5858\n",
      "Epoch 1158/1200\n",
      "3477/3477 [==============================] - 0s 77us/step - loss: 0.3826 - val_loss: 0.5100\n",
      "Epoch 1159/1200\n",
      "3477/3477 [==============================] - 0s 77us/step - loss: 0.3742 - val_loss: 0.5069\n",
      "Epoch 1160/1200\n",
      "3477/3477 [==============================] - 0s 79us/step - loss: 0.3899 - val_loss: 0.5154\n",
      "Epoch 1161/1200\n",
      "3477/3477 [==============================] - 0s 79us/step - loss: 0.3768 - val_loss: 0.5115\n",
      "Epoch 1162/1200\n",
      "3477/3477 [==============================] - 0s 82us/step - loss: 0.3851 - val_loss: 0.5468\n",
      "Epoch 1163/1200\n",
      "3477/3477 [==============================] - 0s 81us/step - loss: 0.3842 - val_loss: 0.5667\n",
      "Epoch 1164/1200\n",
      "3477/3477 [==============================] - 0s 84us/step - loss: 0.3848 - val_loss: 0.5279\n",
      "Epoch 1165/1200\n",
      "3477/3477 [==============================] - 0s 80us/step - loss: 0.3766 - val_loss: 0.5284\n",
      "Epoch 1166/1200\n",
      "3477/3477 [==============================] - 0s 91us/step - loss: 0.3702 - val_loss: 0.5276\n",
      "Epoch 1167/1200\n",
      "3477/3477 [==============================] - 0s 75us/step - loss: 0.3858 - val_loss: 0.5425\n",
      "Epoch 1168/1200\n",
      "3477/3477 [==============================] - 0s 79us/step - loss: 0.3839 - val_loss: 0.5311\n",
      "Epoch 1169/1200\n",
      "3477/3477 [==============================] - 0s 79us/step - loss: 0.3906 - val_loss: 0.5386\n",
      "Epoch 1170/1200\n",
      "3477/3477 [==============================] - 0s 79us/step - loss: 0.3744 - val_loss: 0.5237\n",
      "Epoch 1171/1200\n",
      "3477/3477 [==============================] - 0s 79us/step - loss: 0.3829 - val_loss: 0.5452\n",
      "Epoch 1172/1200\n",
      "3477/3477 [==============================] - 0s 77us/step - loss: 0.3764 - val_loss: 0.5191\n",
      "Epoch 1173/1200\n",
      "3477/3477 [==============================] - 0s 87us/step - loss: 0.3701 - val_loss: 0.5205\n",
      "Epoch 1174/1200\n",
      "3477/3477 [==============================] - 0s 79us/step - loss: 0.3768 - val_loss: 0.5156\n",
      "Epoch 1175/1200\n",
      "3477/3477 [==============================] - 0s 82us/step - loss: 0.3686 - val_loss: 0.5008\n",
      "Epoch 1176/1200\n",
      "3477/3477 [==============================] - 0s 93us/step - loss: 0.3808 - val_loss: 0.5083\n",
      "Epoch 1177/1200\n",
      "3477/3477 [==============================] - 0s 98us/step - loss: 0.3759 - val_loss: 0.5273\n",
      "Epoch 1178/1200\n",
      "3477/3477 [==============================] - 0s 96us/step - loss: 0.3875 - val_loss: 0.5609\n",
      "Epoch 1179/1200\n",
      "3477/3477 [==============================] - 0s 86us/step - loss: 0.3753 - val_loss: 0.5303\n",
      "Epoch 1180/1200\n",
      "3477/3477 [==============================] - 0s 104us/step - loss: 0.3831 - val_loss: 0.5084\n",
      "Epoch 1181/1200\n",
      "3477/3477 [==============================] - 0s 91us/step - loss: 0.3759 - val_loss: 0.5228\n",
      "Epoch 1182/1200\n",
      "3477/3477 [==============================] - 0s 83us/step - loss: 0.3779 - val_loss: 0.5278\n",
      "Epoch 1183/1200\n",
      "3477/3477 [==============================] - 0s 83us/step - loss: 0.3632 - val_loss: 0.5245\n",
      "Epoch 1184/1200\n",
      "3477/3477 [==============================] - 0s 83us/step - loss: 0.3850 - val_loss: 0.6209\n",
      "Epoch 1185/1200\n",
      "3477/3477 [==============================] - 0s 81us/step - loss: 0.3853 - val_loss: 0.5682\n",
      "Epoch 1186/1200\n",
      "3477/3477 [==============================] - 0s 79us/step - loss: 0.3779 - val_loss: 0.5444\n",
      "Epoch 1187/1200\n",
      "3477/3477 [==============================] - 0s 104us/step - loss: 0.3864 - val_loss: 0.5509\n",
      "Epoch 1188/1200\n",
      "3477/3477 [==============================] - 0s 78us/step - loss: 0.3690 - val_loss: 0.5046\n",
      "Epoch 1189/1200\n",
      "3477/3477 [==============================] - 0s 111us/step - loss: 0.3760 - val_loss: 0.5229\n",
      "Epoch 1190/1200\n",
      "3477/3477 [==============================] - 0s 76us/step - loss: 0.3731 - val_loss: 0.5142\n",
      "Epoch 1191/1200\n",
      "3477/3477 [==============================] - 0s 77us/step - loss: 0.3772 - val_loss: 0.5392\n",
      "Epoch 1192/1200\n",
      "3477/3477 [==============================] - 0s 79us/step - loss: 0.3794 - val_loss: 0.5208\n",
      "Epoch 1193/1200\n",
      "3477/3477 [==============================] - 0s 81us/step - loss: 0.3836 - val_loss: 0.5653\n",
      "Epoch 1194/1200\n",
      "3477/3477 [==============================] - 0s 78us/step - loss: 0.3864 - val_loss: 0.5094\n",
      "Epoch 1195/1200\n",
      "3477/3477 [==============================] - 0s 79us/step - loss: 0.3790 - val_loss: 0.5192\n",
      "Epoch 1196/1200\n",
      "3477/3477 [==============================] - 0s 80us/step - loss: 0.3735 - val_loss: 0.5702\n",
      "Epoch 1197/1200\n",
      "3477/3477 [==============================] - 0s 75us/step - loss: 0.3731 - val_loss: 0.5251\n",
      "Epoch 1198/1200\n",
      "3477/3477 [==============================] - 0s 73us/step - loss: 0.3857 - val_loss: 0.5313\n",
      "Epoch 1199/1200\n",
      "3477/3477 [==============================] - 0s 76us/step - loss: 0.3738 - val_loss: 0.5171\n",
      "Epoch 1200/1200\n",
      "3477/3477 [==============================] - 0s 75us/step - loss: 0.3727 - val_loss: 0.5111\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train, y_train, epochs= 1200, batch_size= 32,  validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#loss ê·¸ëž˜í”„ ê·¸ë¦¬ê¸°\n",
    "import matplotlib.pyplot as plt\n",
    "#epochs = range(len(history.history['loss']))\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "\n",
    "plt.legend(['train', 'val'], loc='upper right')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1087/1087 [==============================] - 0s 31us/step\n"
     ]
    }
   ],
   "source": [
    "test_loss = model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4750260489186565"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_loss"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
